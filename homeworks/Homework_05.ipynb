{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "dssc_dl_2021",
   "display_name": "Python (DL virtualenv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Homework 5\n",
    "\n",
    "Starting from the implementation contained within the notebook `05-pruning.ipynb`, extend the  `magnitude_pruning` function to allow for incremental (iterative) pruning. Right now, the magnitude_pruning routine is thought for one-shot pruning. If you try pruning one more time, you'll notice that it will not work as there's no way to communicate to the future calls of magnitude_pruning to ignore the parameters which have already been pruned. Find a way to enhance the routine s.t. it can effectively prune networks in a sequential fashion (i.e., if we passed an MLP already pruned of 20% of its parameters, we want to prune *another* 20% of parameters).\n",
    "\n",
    "Hint: use the mask!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "sys.path.append(\"../labs\")\n",
    "from scripts import mnist, train_utils, architectures, train\n",
    "from scripts.train_utils import accuracy, AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MLPCustom(\n  (layers): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=784, out_features=16, bias=True)\n    (2): ReLU()\n    (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): Linear(in_features=16, out_features=32, bias=True)\n    (5): ReLU()\n    (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Linear(in_features=32, out_features=64, bias=True)\n    (8): ReLU()\n    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): Linear(in_features=64, out_features=10, bias=True)\n    (11): ReLU()\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    {\"n_in\": 784, \"n_out\": 16, \"batchnorm\": False},\n",
    "    {\"n_out\": 32, \"batchnorm\": True},\n",
    "    {\"n_out\": 64, \"batchnorm\": True},\n",
    "    {\"n_out\": 10, \"batchnorm\": True}\n",
    "]\n",
    "net = architectures.MLPCustom(layers)\n",
    "print(net)"
   ]
  },
  {
   "source": [
    "def magnitude_pruning(model, pruning_rate, layers_to_prune=[\"1\", \"4\", \"7\", \"10\"]):\n",
    "    # 1. vectorize distribution of abs(parameter)\n",
    "    '''\n",
    "    model.named_parameters returns a tuple:\n",
    "      element 0 is the name of the parameter\n",
    "      element 1 is the parameter itself\n",
    "    any(<list of booleans>)\n",
    "    checks whether any of the conditions inside the list is True.\n",
    "    [l in pars[0] for l in layers_to_prune] checks whether \"1\", \"4\", \"7\", and \"10\" are inside the parameter name\n",
    "    hence, there will be up to 1 True in the corresponding list: in that case, any(...) returns True\n",
    "    '''\n",
    "    params_to_prune = [pars[1] for pars in model.named_parameters() if any([l in pars[0] for l in layers_to_prune])]\n",
    "    flat = torch.cat([pars.abs().flatten() for pars in params_to_prune], dim=0)\n",
    "\n",
    "    # 2. sort this distribution\n",
    "    flat = flat.sort()[0]\n",
    "\n",
    "    # 3. obtain the threshold\n",
    "    position = int(pruning_rate * flat.shape[0])\n",
    "    thresh = flat[position]\n",
    "\n",
    "    # 4. binarize the parameters & 5. compose these booleans into the mask\n",
    "    # 6. obtain the new structure of parameters\n",
    "    '''\n",
    "    I do this process with a for cycle instead of a list comprehension for clarity\n",
    "    * if the layer is a layer to prune → populate the mask with 1s and 0s\n",
    "    * otherwise → just populate the mask with ones\n",
    "    By doing so, I can immediately apply the mask to the model as well...\n",
    "    '''\n",
    "    mask = []\n",
    "    for pars in model.named_parameters():\n",
    "        if any([l in pars[0] for l in layers_to_prune]):\n",
    "            m = torch.where(pars[1].abs() >= thresh, 1, 0)\n",
    "            mask.append(m)\n",
    "            pars[1].data *= m\n",
    "        else:\n",
    "            mask.append(torch.ones_like(pars[1]))\n",
    "            # no need to multiply as it's all 1s\n",
    "\n",
    "    # 7. what do we need to return?\n",
    "    return mask"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = magnitude_pruning(net, .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_ones_in_mask(mask):\n",
    "    return sum([m.sum().item() for m in mask]) / sum([m.numel() for m in mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of ones in mask: 0.8027967681789931 \n\ntensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1]) \n tensor([ 0.0147, -0.0278,  0.0000, -0.0165, -0.0348, -0.0251, -0.0291,  0.0202,\n        -0.0220, -0.0000, -0.0103,  0.0091,  0.0000, -0.0341, -0.0285, -0.0217,\n         0.0174, -0.0110,  0.0000,  0.0265], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Proportion of parameters which have survived the pruning\n",
    "print(\"Number of ones in mask:\", number_of_ones_in_mask(mask), \"\\n\")\n",
    "# I'm pruning the first 20 parameters of the network and the mask to see if it actually works\n",
    "print(mask[0][0,:20], \"\\n\", next(net.parameters())[0,:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader, _, _ = mnist.get_data(data_root=\"../labs/datasets/\")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# load pretrained model\n",
    "state_dict = torch.load(\"../labs/models_push/mlp_custom_mnist/mlp_custom_mnist.pt\")\n",
    "net.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/angela/Documenti/Deep Learning/DSSC_DL_2021/venv/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "TESTING - loss 7349.624471664429 - performance 0.9631\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(7349.624471664429, 0.9631333333333333)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "train.test_model(net, testloader, loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7535114978247358"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "mask = magnitude_pruning(net, pruning_rate=0.25)\n",
    "print(\"Number of ones in mask:\", number_of_ones_in_mask(mask), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TESTING - loss 7674.807939052582 - performance 0.9612\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(7674.807939052582, 0.9612333333333334)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "train.test_model(net, testloader, loss_fn=loss_fn)"
   ]
  },
  {
   "source": [
    "We see that there's a drop in performance. This is a characteristic of pruning. Rarely a simple application of pruning leaves the performance untouched or better. Due to that, we must procede with a re-training of the ANN."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device, mask, layers_to_prune=[\"1\", \"4\", \"7\", \"10\"]):\n",
    "    for X, y in dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        y_hat = model(X)\n",
    "\n",
    "        loss = loss_fn(y_hat, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        ##### we must neutralize the gradient on the pruned params before the optimizer takes a step ####\n",
    "\n",
    "        if mask is not None:\n",
    "            for (name, param), m in zip(model.named_parameters(), mask):\n",
    "                if any([l in name for l in layers_to_prune]):\n",
    "                    param.grad *= m\n",
    "\n",
    "        ######\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = performance(y_hat, y)\n",
    "\n",
    "        loss_meter.update(val=loss.item(), n=X.shape[0])\n",
    "        performance_meter.update(val=acc, n=X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, loss_fn, optimizer, num_epochs, checkpoint_loc=None, checkpoint_name=\"checkpoint.pt\", performance=accuracy, lr_scheduler=None, device=None, mask=None, params_type_to_prune=[\"weight\", \"bias\"]):\n",
    "    if checkpoint_loc is not None:\n",
    "        os.makedirs(checkpoint_loc, exist_ok=True)\n",
    "\n",
    "    if device is None:\n",
    "        device = use_gpu_if_possible()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_meter = AverageMeter()\n",
    "        performance_meter = AverageMeter()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} --- learning rate {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "\n",
    "        train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device, mask)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum} - average: {loss_meter.avg}; Performance: {performance_meter.avg}\")\n",
    "\n",
    "        if checkpoint_name is not None and checkpoint_loc is not None:\n",
    "            checkpoint_dict = {\n",
    "                \"parameters\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            torch.save(checkpoint_dict, os.path.join(checkpoint_loc, checkpoint_name))\n",
    "        \n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    return loss_meter.sum, performance_meter.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 --- learning rate 0.01000\n",
      "Epoch 1 completed. Loss - total: 12252.428242206573 - average: 0.20420713737010956; Performance: 0.9375666666666667\n",
      "Epoch 2 --- learning rate 0.01000\n",
      "Epoch 2 completed. Loss - total: 9087.08480644226 - average: 0.15145141344070434; Performance: 0.9534833333333333\n",
      "Epoch 3 --- learning rate 0.01000\n",
      "Epoch 3 completed. Loss - total: 8068.963097572327 - average: 0.13448271829287212; Performance: 0.9578833333333333\n",
      "Epoch 4 --- learning rate 0.01000\n",
      "Epoch 4 completed. Loss - total: 7334.235938549042 - average: 0.12223726564248404; Performance: 0.9618\n",
      "Epoch 5 --- learning rate 0.01000\n",
      "Epoch 5 completed. Loss - total: 6821.295444488525 - average: 0.11368825740814209; Performance: 0.9642666666666667\n",
      "Epoch 6 --- learning rate 0.01000\n",
      "Epoch 6 completed. Loss - total: 6592.9484832286835 - average: 0.10988247472047806; Performance: 0.9655\n",
      "Epoch 7 --- learning rate 0.01000\n",
      "Epoch 7 completed. Loss - total: 6120.685695171356 - average: 0.10201142825285593; Performance: 0.9673666666666667\n",
      "Epoch 8 --- learning rate 0.01000\n",
      "Epoch 8 completed. Loss - total: 5850.153789997101 - average: 0.09750256316661834; Performance: 0.9691166666666666\n",
      "Epoch 9 --- learning rate 0.01000\n",
      "Epoch 9 completed. Loss - total: 5597.987704277039 - average: 0.09329979507128397; Performance: 0.9702833333333334\n",
      "Epoch 10 --- learning rate 0.01000\n",
      "Epoch 10 completed. Loss - total: 5353.642367362976 - average: 0.08922737278938293; Performance: 0.9707333333333333\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5353.642367362976, 0.9707333333333333)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "lr = 0.01\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "\n",
    "train_model(net, trainloader, loss_fn, optimizer, num_epochs, device=\"cpu\", mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TESTING - loss 3899.6647787094116 - performance 0.9791\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3899.6647787094116, 0.9790666666666666)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "train.test_model(net, testloader, loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7535114978247358"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "mask = magnitude_pruning(net, pruning_rate=0.25)\n",
    "print(\"Number of ones in mask:\", number_of_ones_in_mask(mask), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TESTING - loss 3899.6647787094116 - performance 0.9791\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3899.6647787094116, 0.9790666666666666)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "train.test_model(net, testloader, loss_fn=loss_fn)"
   ]
  },
  {
   "source": [
    "We can see that the number of ones in the mask is the same as before! So we couldn't prune another 25% of the parameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Iterative Magnitude Pruning (IMP)\n",
    "\n",
    "To be able to prune again the parameters in the network, we compute the threshold removing the parameters that are already zero, so that we compute the pruning rate only on the non-zero parameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude_pruning(model, pruning_rate, layers_to_prune=[\"1\", \"4\", \"7\", \"10\"]):\n",
    "    # 1. vectorize distribution of abs(parameter)\n",
    "    '''\n",
    "    model.named_parameters returns a tuple:\n",
    "      element 0 is the name of the parameter\n",
    "      element 1 is the parameter itself\n",
    "    any(<list of booleans>)\n",
    "    checks whether any of the conditions inside the list is True.\n",
    "    [l in pars[0] for l in layers_to_prune] checks whether \"1\", \"4\", \"7\", and \"10\" are inside the parameter name\n",
    "    hence, there will be up to 1 True in the corresponding list: in that case, any(...) returns True\n",
    "    '''\n",
    "    params_to_prune = [pars[1] for pars in model.named_parameters() if any([l in pars[0] for l in layers_to_prune])]\n",
    "    flat = torch.cat([pars.abs().flatten() for pars in params_to_prune], dim=0)\n",
    "    print(\"Parameters dimensions: \", end=\"\")\n",
    "    print(flat.shape)\n",
    "\n",
    "    # 2. sort this distribution\n",
    "    flat = flat.sort()[0]\n",
    "\n",
    "    # 2.5. remove the zeros, thus the parameters already pruned\n",
    "    flat = flat[flat.nonzero().detach()]\n",
    "    print(\"Pruned parameters dimensions: \", end=\"\")\n",
    "    print(flat.shape)\n",
    "\n",
    "    # 3. obtain the threshold\n",
    "    position = int(pruning_rate * flat.shape[0])\n",
    "    thresh = flat[position]\n",
    "\n",
    "    # 4. binarize the parameters & 5. compose these booleans into the mask\n",
    "    # 6. obtain the new structure of parameters\n",
    "    '''\n",
    "    I do this process with a for cycle instead of a list comprehension for clarity\n",
    "    * if the layer is a layer to prune → populate the mask with 1s and 0s\n",
    "    * otherwise → just populate the mask with ones\n",
    "    By doing so, I can immediately apply the mask to the model as well...\n",
    "    '''\n",
    "    mask = []\n",
    "    for pars in model.named_parameters():\n",
    "        if any([l in pars[0] for l in layers_to_prune]):\n",
    "            m = torch.where(pars[1].abs() >= thresh, 1, 0)\n",
    "            mask.append(m)\n",
    "            pars[1].data *= m\n",
    "        else:\n",
    "            mask.append(torch.ones_like(pars[1]))\n",
    "            # no need to multiply as it's all 1s\n",
    "\n",
    "    # 7. what do we need to return?\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TESTING - loss 7349.624471664429 - performance 0.9631\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(7349.624471664429, 0.9631333333333333)"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "# load pretrained model\n",
    "state_dict = torch.load(\"../labs/models_push/mlp_custom_mnist/mlp_custom_mnist.pt\")\n",
    "net.load_state_dict(state_dict)\n",
    "\n",
    "train.test_model(net, testloader, loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1.\n",
      "\n",
      "Parameters dimensions: torch.Size([15866])\n",
      "Pruned parameters dimensions: torch.Size([15866, 1])\n",
      "Number of ones in mask: 0.7535114978247358 \n",
      "\n",
      "Epoch 1 --- learning rate 0.01000\n",
      "Epoch 1 completed. Loss - total: 12903.265724182129 - average: 0.21505442873636882; Performance: 0.93435\n",
      "Epoch 2 --- learning rate 0.01000\n",
      "Epoch 2 completed. Loss - total: 9346.30019569397 - average: 0.15577166992823283; Performance: 0.9516166666666667\n",
      "Epoch 3 --- learning rate 0.01000\n",
      "Epoch 3 completed. Loss - total: 7972.33712720871 - average: 0.13287228545347848; Performance: 0.9576666666666667\n",
      "Epoch 4 --- learning rate 0.01000\n",
      "Epoch 4 completed. Loss - total: 7332.6753578186035 - average: 0.12221125596364339; Performance: 0.9619166666666666\n",
      "Epoch 5 --- learning rate 0.01000\n",
      "Epoch 5 completed. Loss - total: 6764.026443362236 - average: 0.11273377405603727; Performance: 0.9646\n",
      "Epoch 6 --- learning rate 0.01000\n",
      "Epoch 6 completed. Loss - total: 6354.145076751709 - average: 0.10590241794586182; Performance: 0.9662333333333334\n",
      "Epoch 7 --- learning rate 0.01000\n",
      "Epoch 7 completed. Loss - total: 6103.41014289856 - average: 0.10172350238164266; Performance: 0.9679\n",
      "Epoch 8 --- learning rate 0.01000\n",
      "Epoch 8 completed. Loss - total: 5627.860759735107 - average: 0.09379767932891846; Performance: 0.9701833333333333\n",
      "Epoch 9 --- learning rate 0.01000\n",
      "Epoch 9 completed. Loss - total: 5453.399039745331 - average: 0.09088998399575551; Performance: 0.971\n",
      "Epoch 10 --- learning rate 0.01000\n",
      "Epoch 10 completed. Loss - total: 5401.867142558098 - average: 0.09003111904263496; Performance: 0.9707666666666667\n",
      "TESTING - loss 4711.27717089653 - performance 0.9745\n",
      "\n",
      "\n",
      "Iteration 2.\n",
      "\n",
      "Parameters dimensions: torch.Size([15866])\n",
      "Pruned parameters dimensions: torch.Size([11900, 1])\n",
      "Number of ones in mask: 0.5686140459912989 \n",
      "\n",
      "Epoch 1 --- learning rate 0.01000\n",
      "Epoch 1 completed. Loss - total: 5477.281371355057 - average: 0.09128802285591761; Performance: 0.9705\n",
      "Epoch 2 --- learning rate 0.01000\n",
      "Epoch 2 completed. Loss - total: 5049.985221385956 - average: 0.0841664203564326; Performance: 0.9726333333333333\n",
      "Epoch 3 --- learning rate 0.01000\n",
      "Epoch 3 completed. Loss - total: 4836.276056170464 - average: 0.08060460093617439; Performance: 0.9738166666666667\n",
      "Epoch 4 --- learning rate 0.01000\n",
      "Epoch 4 completed. Loss - total: 4611.311486959457 - average: 0.07685519144932429; Performance: 0.9747166666666667\n",
      "Epoch 5 --- learning rate 0.01000\n",
      "Epoch 5 completed. Loss - total: 4580.624465703964 - average: 0.07634374109506607; Performance: 0.9751166666666666\n",
      "Epoch 6 --- learning rate 0.01000\n",
      "Epoch 6 completed. Loss - total: 4287.51044011116 - average: 0.07145850733518601; Performance: 0.9766166666666667\n",
      "Epoch 7 --- learning rate 0.01000\n",
      "Epoch 7 completed. Loss - total: 4234.298186779022 - average: 0.07057163644631703; Performance: 0.9770666666666666\n",
      "Epoch 8 --- learning rate 0.01000\n",
      "Epoch 8 completed. Loss - total: 4176.626234292984 - average: 0.0696104372382164; Performance: 0.9766833333333333\n",
      "Epoch 9 --- learning rate 0.01000\n",
      "Epoch 9 completed. Loss - total: 4078.1438373327255 - average: 0.06796906395554543; Performance: 0.9777666666666667\n",
      "Epoch 10 --- learning rate 0.01000\n",
      "Epoch 10 completed. Loss - total: 4111.733102321625 - average: 0.06852888503869374; Performance: 0.97705\n",
      "TESTING - loss 2947.7427995204926 - performance 0.9839\n",
      "\n",
      "\n",
      "Iteration 3.\n",
      "\n",
      "Parameters dimensions: torch.Size([15866])\n",
      "Pruned parameters dimensions: torch.Size([8925, 1])\n",
      "Number of ones in mask: 0.42995649471721564 \n",
      "\n",
      "Epoch 1 --- learning rate 0.01000\n",
      "Epoch 1 completed. Loss - total: 4106.7361669540405 - average: 0.06844560278256734; Performance: 0.9769333333333333\n",
      "Epoch 2 --- learning rate 0.01000\n",
      "Epoch 2 completed. Loss - total: 3693.5781197547913 - average: 0.06155963532924652; Performance: 0.97955\n",
      "Epoch 3 --- learning rate 0.01000\n",
      "Epoch 3 completed. Loss - total: 3520.0219802856445 - average: 0.05866703300476074; Performance: 0.9801166666666666\n",
      "Epoch 4 --- learning rate 0.01000\n",
      "Epoch 4 completed. Loss - total: 3499.360523700714 - average: 0.0583226753950119; Performance: 0.98\n",
      "Epoch 5 --- learning rate 0.01000\n",
      "Epoch 5 completed. Loss - total: 3498.0106585025787 - average: 0.058300177641709645; Performance: 0.98035\n",
      "Epoch 6 --- learning rate 0.01000\n",
      "Epoch 6 completed. Loss - total: 3318.0934920310974 - average: 0.05530155820051829; Performance: 0.9813\n",
      "Epoch 7 --- learning rate 0.01000\n",
      "Epoch 7 completed. Loss - total: 3347.427262067795 - average: 0.05579045436779658; Performance: 0.9809833333333333\n",
      "Epoch 8 --- learning rate 0.01000\n",
      "Epoch 8 completed. Loss - total: 3174.8962020874023 - average: 0.05291493670145671; Performance: 0.9819166666666667\n",
      "Epoch 9 --- learning rate 0.01000\n",
      "Epoch 9 completed. Loss - total: 3036.0994803905487 - average: 0.05060165800650915; Performance: 0.9826\n",
      "Epoch 10 --- learning rate 0.01000\n",
      "Epoch 10 completed. Loss - total: 3118.8631360530853 - average: 0.051981052267551425; Performance: 0.9824833333333334\n",
      "TESTING - loss 2411.3800616264343 - performance 0.9867\n",
      "\n",
      "\n",
      "Iteration 4.\n",
      "\n",
      "Parameters dimensions: torch.Size([15866])\n",
      "Pruned parameters dimensions: torch.Size([6694, 1])\n",
      "Number of ones in mask: 0.3259788688626476 \n",
      "\n",
      "Epoch 1 --- learning rate 0.01000\n",
      "Epoch 1 completed. Loss - total: 4090.3626247644424 - average: 0.06817271041274071; Performance: 0.97675\n",
      "Epoch 2 --- learning rate 0.01000\n",
      "Epoch 2 completed. Loss - total: 3466.2646295428276 - average: 0.05777107715904713; Performance: 0.98025\n",
      "Epoch 3 --- learning rate 0.01000\n",
      "Epoch 3 completed. Loss - total: 3273.676102876663 - average: 0.05456126838127772; Performance: 0.9808833333333333\n",
      "Epoch 4 --- learning rate 0.01000\n",
      "Epoch 4 completed. Loss - total: 3288.091831445694 - average: 0.0548015305240949; Performance: 0.9809166666666667\n",
      "Epoch 5 --- learning rate 0.01000\n",
      "Epoch 5 completed. Loss - total: 3124.463756799698 - average: 0.052074395946661634; Performance: 0.98195\n",
      "Epoch 6 --- learning rate 0.01000\n",
      "Epoch 6 completed. Loss - total: 3119.9047405719757 - average: 0.051998412342866264; Performance: 0.9824666666666667\n",
      "Epoch 7 --- learning rate 0.01000\n",
      "Epoch 7 completed. Loss - total: 3184.685392856598 - average: 0.0530780898809433; Performance: 0.9814333333333334\n",
      "Epoch 8 --- learning rate 0.01000\n",
      "Epoch 8 completed. Loss - total: 3100.018208026886 - average: 0.05166697013378143; Performance: 0.9822666666666666\n",
      "Epoch 9 --- learning rate 0.01000\n",
      "Epoch 9 completed. Loss - total: 3044.5243022441864 - average: 0.050742071704069774; Performance: 0.9827333333333333\n",
      "Epoch 10 --- learning rate 0.01000\n",
      "Epoch 10 completed. Loss - total: 3025.333993911743 - average: 0.05042223323186239; Performance: 0.9827166666666667\n",
      "TESTING - loss 2149.8116643428802 - performance 0.9877\n",
      "\n",
      "\n",
      "Iteration 5.\n",
      "\n",
      "Parameters dimensions: torch.Size([15866])\n",
      "Pruned parameters dimensions: torch.Size([5021, 1])\n",
      "Number of ones in mask: 0.24798011187072716 \n",
      "\n",
      "Epoch 1 --- learning rate 0.01000\n",
      "Epoch 1 completed. Loss - total: 5815.483922958374 - average: 0.09692473204930624; Performance: 0.9684833333333334\n",
      "Epoch 2 --- learning rate 0.01000\n",
      "Epoch 2 completed. Loss - total: 4387.324797153473 - average: 0.07312207995255789; Performance: 0.9747166666666667\n",
      "Epoch 3 --- learning rate 0.01000\n",
      "Epoch 3 completed. Loss - total: 4280.561831474304 - average: 0.0713426971912384; Performance: 0.9753833333333334\n",
      "Epoch 4 --- learning rate 0.01000\n",
      "Epoch 4 completed. Loss - total: 4034.2567415237427 - average: 0.06723761235872905; Performance: 0.9771333333333333\n",
      "Epoch 5 --- learning rate 0.01000\n",
      "Epoch 5 completed. Loss - total: 3940.2217779159546 - average: 0.0656703629652659; Performance: 0.9779333333333333\n",
      "Epoch 6 --- learning rate 0.01000\n",
      "Epoch 6 completed. Loss - total: 3912.5732769966125 - average: 0.06520955461661021; Performance: 0.97725\n",
      "Epoch 7 --- learning rate 0.01000\n",
      "Epoch 7 completed. Loss - total: 3795.2397763729095 - average: 0.06325399627288182; Performance: 0.9784666666666667\n",
      "Epoch 8 --- learning rate 0.01000\n",
      "Epoch 8 completed. Loss - total: 3824.8068811893463 - average: 0.06374678135315577; Performance: 0.9779666666666667\n",
      "Epoch 9 --- learning rate 0.01000\n",
      "Epoch 9 completed. Loss - total: 3765.2554183602333 - average: 0.06275425697267055; Performance: 0.9785666666666667\n",
      "Epoch 10 --- learning rate 0.01000\n",
      "Epoch 10 completed. Loss - total: 3702.656962633133 - average: 0.06171094937721888; Performance: 0.9788166666666667\n",
      "TESTING - loss 2990.127831697464 - performance 0.9828\n",
      "\n",
      "\n",
      "Iteration 6.\n",
      "\n",
      "Parameters dimensions: torch.Size([15866])\n",
      "Pruned parameters dimensions: torch.Size([3766, 1])\n",
      "Number of ones in mask: 0.18949658172778122 \n",
      "\n",
      "Epoch 1 --- learning rate 0.01000\n",
      "Epoch 1 completed. Loss - total: 8026.796125650406 - average: 0.13377993542750677; Performance: 0.9563833333333334\n",
      "Epoch 2 --- learning rate 0.01000\n",
      "Epoch 2 completed. Loss - total: 6066.827267765999 - average: 0.10111378779609997; Performance: 0.9672833333333334\n",
      "Epoch 3 --- learning rate 0.01000\n",
      "Epoch 3 completed. Loss - total: 5702.671354293823 - average: 0.09504452257156372; Performance: 0.9693333333333334\n",
      "Epoch 4 --- learning rate 0.01000\n",
      "Epoch 4 completed. Loss - total: 5562.8635630607605 - average: 0.09271439271767934; Performance: 0.9695166666666667\n",
      "Epoch 5 --- learning rate 0.01000\n",
      "Epoch 5 completed. Loss - total: 5449.272178649902 - average: 0.09082120297749838; Performance: 0.9705333333333334\n",
      "Epoch 6 --- learning rate 0.01000\n",
      "Epoch 6 completed. Loss - total: 5296.625181913376 - average: 0.08827708636522293; Performance: 0.9712166666666666\n",
      "Epoch 7 --- learning rate 0.01000\n",
      "Epoch 7 completed. Loss - total: 5324.83070731163 - average: 0.08874717845519384; Performance: 0.9708333333333333\n",
      "Epoch 8 --- learning rate 0.01000\n",
      "Epoch 8 completed. Loss - total: 5231.7672798633575 - average: 0.08719612133105596; Performance: 0.9714666666666667\n",
      "Epoch 9 --- learning rate 0.01000\n",
      "Epoch 9 completed. Loss - total: 5270.681496143341 - average: 0.08784469160238902; Performance: 0.9713166666666667\n",
      "Epoch 10 --- learning rate 0.01000\n",
      "Epoch 10 completed. Loss - total: 5036.072541117668 - average: 0.08393454235196114; Performance: 0.9726666666666667\n",
      "TESTING - loss 4282.554399251938 - performance 0.9762\n",
      "\n",
      "\n",
      "Iteration 7.\n",
      "\n",
      "Parameters dimensions: torch.Size([15866])\n",
      "Pruned parameters dimensions: torch.Size([2825, 1])\n",
      "Number of ones in mask: 0.14561839651957736 \n",
      "\n",
      "Epoch 1 --- learning rate 0.01000\n",
      "Epoch 1 completed. Loss - total: 9731.790531158447 - average: 0.16219650885264078; Performance: 0.9486\n",
      "Epoch 2 --- learning rate 0.01000\n",
      "Epoch 2 completed. Loss - total: 7967.433979034424 - average: 0.1327905663172404; Performance: 0.9584\n",
      "Epoch 3 --- learning rate 0.01000\n",
      "Epoch 3 completed. Loss - total: 7505.744069933891 - average: 0.1250957344988982; Performance: 0.9601166666666666\n",
      "Epoch 4 --- learning rate 0.01000\n",
      "Epoch 4 completed. Loss - total: 7261.805760860443 - average: 0.12103009601434071; Performance: 0.9609333333333333\n",
      "Epoch 5 --- learning rate 0.01000\n",
      "Epoch 5 completed. Loss - total: 7055.7564833164215 - average: 0.11759594138860703; Performance: 0.9619833333333333\n",
      "Epoch 6 --- learning rate 0.01000\n",
      "Epoch 6 completed. Loss - total: 6950.611399173737 - average: 0.11584352331956227; Performance: 0.9618833333333333\n",
      "Epoch 7 --- learning rate 0.01000\n",
      "Epoch 7 completed. Loss - total: 6932.397282123566 - average: 0.11553995470205942; Performance: 0.96295\n",
      "Epoch 8 --- learning rate 0.01000\n",
      "Epoch 8 completed. Loss - total: 6935.630017757416 - average: 0.11559383362929027; Performance: 0.9626166666666667\n",
      "Epoch 9 --- learning rate 0.01000\n",
      "Epoch 9 completed. Loss - total: 6794.037310123444 - average: 0.11323395516872406; Performance: 0.9637\n",
      "Epoch 10 --- learning rate 0.01000\n",
      "Epoch 10 completed. Loss - total: 6728.077575206757 - average: 0.11213462625344595; Performance: 0.9632166666666667\n",
      "TESTING - loss 5968.607303857803 - performance 0.9676\n",
      "\n",
      "\n",
      "Iteration 8.\n",
      "\n",
      "Parameters dimensions: torch.Size([15866])\n",
      "Pruned parameters dimensions: torch.Size([2119, 1])\n",
      "Number of ones in mask: 0.1127408328154133 \n",
      "\n",
      "Epoch 1 --- learning rate 0.01000\n",
      "Epoch 1 completed. Loss - total: 15498.860340118408 - average: 0.2583143390019735; Performance: 0.9195666666666666\n",
      "Epoch 2 --- learning rate 0.01000\n",
      "Epoch 2 completed. Loss - total: 11373.864292144775 - average: 0.18956440486907958; Performance: 0.9414333333333333\n",
      "Epoch 3 --- learning rate 0.01000\n",
      "Epoch 3 completed. Loss - total: 10785.785450458527 - average: 0.17976309084097544; Performance: 0.9441833333333334\n",
      "Epoch 4 --- learning rate 0.01000\n",
      "Epoch 4 completed. Loss - total: 10391.347464561462 - average: 0.17318912440935771; Performance: 0.9456833333333333\n",
      "Epoch 5 --- learning rate 0.01000\n",
      "Epoch 5 completed. Loss - total: 10243.696511268616 - average: 0.17072827518781025; Performance: 0.9465833333333333\n",
      "Epoch 6 --- learning rate 0.01000\n",
      "Epoch 6 completed. Loss - total: 10042.150240421295 - average: 0.16736917067368826; Performance: 0.9473333333333334\n",
      "Epoch 7 --- learning rate 0.01000\n",
      "Epoch 7 completed. Loss - total: 9901.791090011597 - average: 0.16502985150019328; Performance: 0.9476666666666667\n",
      "Epoch 8 --- learning rate 0.01000\n",
      "Epoch 8 completed. Loss - total: 9785.675386428833 - average: 0.16309458977381389; Performance: 0.94935\n",
      "Epoch 9 --- learning rate 0.01000\n",
      "Epoch 9 completed. Loss - total: 9695.03174161911 - average: 0.1615838623603185; Performance: 0.9498333333333333\n",
      "Epoch 10 --- learning rate 0.01000\n",
      "Epoch 10 completed. Loss - total: 9609.839015960693 - average: 0.1601639835993449; Performance: 0.9499166666666666\n",
      "TESTING - loss 8750.741907119751 - performance 0.9537\n",
      "\n",
      "\n",
      "Iteration 9.\n",
      "\n",
      "Parameters dimensions: torch.Size([15866])\n",
      "Pruned parameters dimensions: torch.Size([1590, 1])\n",
      "Number of ones in mask: 0.08806712243629583 \n",
      "\n",
      "Epoch 1 --- learning rate 0.01000\n",
      "Epoch 1 completed. Loss - total: 21211.666089057922 - average: 0.35352776815096537; Performance: 0.8942166666666667\n",
      "Epoch 2 --- learning rate 0.01000\n",
      "Epoch 2 completed. Loss - total: 16627.78769493103 - average: 0.27712979491551715; Performance: 0.9179333333333334\n",
      "Epoch 3 --- learning rate 0.01000\n",
      "Epoch 3 completed. Loss - total: 15757.350507736206 - average: 0.2626225084622701; Performance: 0.9208666666666666\n",
      "Epoch 4 --- learning rate 0.01000\n",
      "Epoch 4 completed. Loss - total: 15244.186298131943 - average: 0.2540697716355324; Performance: 0.9249333333333334\n",
      "Epoch 5 --- learning rate 0.01000\n",
      "Epoch 5 completed. Loss - total: 14973.8845038414 - average: 0.24956474173069002; Performance: 0.92485\n",
      "Epoch 6 --- learning rate 0.01000\n",
      "Epoch 6 completed. Loss - total: 14804.965802192688 - average: 0.2467494300365448; Performance: 0.9261666666666667\n",
      "Epoch 7 --- learning rate 0.01000\n",
      "Epoch 7 completed. Loss - total: 14526.232383728027 - average: 0.2421038730621338; Performance: 0.9276333333333333\n",
      "Epoch 8 --- learning rate 0.01000\n",
      "Epoch 8 completed. Loss - total: 14551.330187797546 - average: 0.24252216979662578; Performance: 0.9272333333333334\n",
      "Epoch 9 --- learning rate 0.01000\n",
      "Epoch 9 completed. Loss - total: 14452.483984947205 - average: 0.2408747330824534; Performance: 0.92825\n",
      "Epoch 10 --- learning rate 0.01000\n",
      "Epoch 10 completed. Loss - total: 14304.871256828308 - average: 0.23841452094713847; Performance: 0.9282666666666667\n",
      "TESTING - loss 13558.773228168488 - performance 0.9326\n",
      "\n",
      "\n",
      "Iteration 10.\n",
      "\n",
      "Parameters dimensions: torch.Size([15866])\n",
      "Pruned parameters dimensions: torch.Size([1193, 1])\n",
      "Number of ones in mask: 0.06954630205096334 \n",
      "\n",
      "Epoch 1 --- learning rate 0.01000\n",
      "Epoch 1 completed. Loss - total: 34522.15001487732 - average: 0.575369166914622; Performance: 0.8237333333333333\n",
      "Epoch 2 --- learning rate 0.01000\n",
      "Epoch 2 completed. Loss - total: 27196.17173576355 - average: 0.4532695289293925; Performance: 0.8591333333333333\n",
      "Epoch 3 --- learning rate 0.01000\n",
      "Epoch 3 completed. Loss - total: 26011.081241607666 - average: 0.4335180206934611; Performance: 0.8656833333333334\n",
      "Epoch 4 --- learning rate 0.01000\n",
      "Epoch 4 completed. Loss - total: 25424.110661506653 - average: 0.42373517769177754; Performance: 0.8687666666666667\n",
      "Epoch 5 --- learning rate 0.01000\n",
      "Epoch 5 completed. Loss - total: 25028.951338768005 - average: 0.41714918897946673; Performance: 0.8716666666666667\n",
      "Epoch 6 --- learning rate 0.01000\n",
      "Epoch 6 completed. Loss - total: 24680.321442604065 - average: 0.41133869071006773; Performance: 0.8723333333333333\n",
      "Epoch 7 --- learning rate 0.01000\n",
      "Epoch 7 completed. Loss - total: 24514.607449531555 - average: 0.4085767908255259; Performance: 0.8732833333333333\n",
      "Epoch 8 --- learning rate 0.01000\n",
      "Epoch 8 completed. Loss - total: 24319.865339279175 - average: 0.40533108898798625; Performance: 0.87545\n",
      "Epoch 9 --- learning rate 0.01000\n",
      "Epoch 9 completed. Loss - total: 24428.51418876648 - average: 0.407141903146108; Performance: 0.87485\n",
      "Epoch 10 --- learning rate 0.01000\n",
      "Epoch 10 completed. Loss - total: 24219.204233169556 - average: 0.4036534038861593; Performance: 0.8752666666666666\n",
      "TESTING - loss 368400637.390625 - performance 0.0986\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Iteration {i+1}.\\n\")\n",
    "    mask = magnitude_pruning(net, pruning_rate=0.25)\n",
    "    print(\"Number of ones in mask:\", number_of_ones_in_mask(mask), \"\\n\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "    train_model(net, trainloader, loss_fn, optimizer, num_epochs, device=\"cpu\", mask=mask)\n",
    "    train.test_model(net, testloader, loss_fn=loss_fn)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "source": [
    "We can see that 10 iterations of IMP result in a final sparsity of the ANN of about $0.75^{10}\\approx 0.0563$ (ours is a bit higher since we prune only the parameters in the Linear layers and not in the BatchNorm layers).\n",
    "\n",
    "We can also notice that with iteration 5 the performance starts to drop, expecially in the last iteration where we have a drammatic drop to a test accuracy of $0.0986$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}