{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "dssc_dl_2021",
   "display_name": "Python (DL virtualenv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Homework 5\n",
    "\n",
    "Starting from the implementation contained within the notebook `05-pruning.ipynb`, extend the  `magnitude_pruning` function to allow for incremental (iterative) pruning. Right now, the magnitude_pruning routine is thought for one-shot pruning. If you try pruning one more time, you'll notice that it will not work as there's no way to communicate to the future calls of magnitude_pruning to ignore the parameters which have already been pruned. Find a way to enhance the routine s.t. it can effectively prune networks in a sequential fashion (i.e., if we passed an MLP already pruned of 20% of its parameters, we want to prune *another* 20% of parameters).\n",
    "\n",
    "Hint: use the mask!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "sys.path.append(\"../labs\")\n",
    "from scripts import mnist, train_utils, architectures, train\n",
    "from scripts.train_utils import accuracy, AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MLPCustom(\n  (layers): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=784, out_features=16, bias=True)\n    (2): ReLU()\n    (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): Linear(in_features=16, out_features=32, bias=True)\n    (5): ReLU()\n    (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Linear(in_features=32, out_features=64, bias=True)\n    (8): ReLU()\n    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): Linear(in_features=64, out_features=10, bias=True)\n    (11): ReLU()\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    {\"n_in\": 784, \"n_out\": 16, \"batchnorm\": False},\n",
    "    {\"n_out\": 32, \"batchnorm\": True},\n",
    "    {\"n_out\": 64, \"batchnorm\": True},\n",
    "    {\"n_out\": 10, \"batchnorm\": True}\n",
    "]\n",
    "net = architectures.MLPCustom(layers)\n",
    "print(net)"
   ]
  },
  {
   "source": [
    "def magnitude_pruning(model, pruning_rate, layers_to_prune=[\"1\", \"4\", \"7\", \"10\"]):\n",
    "    # 1. vectorize distribution of abs(parameter)\n",
    "    '''\n",
    "    model.named_parameters returns a tuple:\n",
    "      element 0 is the name of the parameter\n",
    "      element 1 is the parameter itself\n",
    "    any(<list of booleans>)\n",
    "    checks whether any of the conditions inside the list is True.\n",
    "    [l in pars[0] for l in layers_to_prune] checks whether \"1\", \"4\", \"7\", and \"10\" are inside the parameter name\n",
    "    hence, there will be up to 1 True in the corresponding list: in that case, any(...) returns True\n",
    "    '''\n",
    "    params_to_prune = [pars[1] for pars in model.named_parameters() if any([l in pars[0] for l in layers_to_prune])]\n",
    "    flat = torch.cat([pars.abs().flatten() for pars in params_to_prune], dim=0)\n",
    "\n",
    "    # 2. sort this distribution\n",
    "    flat = flat.sort()[0]\n",
    "\n",
    "    # 3. obtain the threshold\n",
    "    position = int(pruning_rate * flat.shape[0])\n",
    "    thresh = flat[position]\n",
    "\n",
    "    # 4. binarize the parameters & 5. compose these booleans into the mask\n",
    "    # 6. obtain the new structure of parameters\n",
    "    '''\n",
    "    I do this process with a for cycle instead of a list comprehension for clarity\n",
    "    * if the layer is a layer to prune → populate the mask with 1s and 0s\n",
    "    * otherwise → just populate the mask with ones\n",
    "    By doing so, I can immediately apply the mask to the model as well...\n",
    "    '''\n",
    "    mask = []\n",
    "    for pars in model.named_parameters():\n",
    "        if any([l in pars[0] for l in layers_to_prune]):\n",
    "            m = torch.where(pars[1].abs() >= thresh, 1, 0)\n",
    "            mask.append(m)\n",
    "            pars[1].data *= m\n",
    "        else:\n",
    "            mask.append(torch.ones_like(pars[1]))\n",
    "            # no need to multiply as it's all 1s\n",
    "\n",
    "    # 7. what do we need to return?\n",
    "    return mask"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = magnitude_pruning(net, .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_ones_in_mask(mask):\n",
    "    return sum([m.sum().item() for m in mask]) / sum([m.numel() for m in mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of ones in mask: 0.8027967681789931 \n\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1]) \n tensor([-0.0176, -0.0233, -0.0310, -0.0124,  0.0158,  0.0109,  0.0300,  0.0117,\n         0.0000,  0.0354,  0.0111, -0.0331,  0.0326, -0.0229, -0.0000, -0.0000,\n         0.0000,  0.0190, -0.0000,  0.0256], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Proportion of parameters which have survived the pruning\n",
    "print(\"Number of ones in mask:\", number_of_ones_in_mask(mask), \"\\n\")\n",
    "# I'm pruning the first 20 parameters of the network and the mask to see if it actually works\n",
    "print(mask[0][0,:20], \"\\n\", next(net.parameters())[0,:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader, _, _ = mnist.get_data(data_root=\"../labs/datasets/\")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# load pretrained model\n",
    "state_dict = torch.load(\"../labs/models_push/mlp_custom_mnist/mlp_custom_mnist.pt\")\n",
    "net.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/angela/Documenti/Deep Learning/DSSC_DL_2021/venv/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "TESTING - loss 7349.624471664429 - performance 0.9631\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(7349.624471664429, 0.9631333333333333)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "train.test_model(net, testloader, loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5069608452454941"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "mask = magnitude_pruning(net, pruning_rate=0.5)\n",
    "number_of_ones_in_mask(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TESTING - loss 9346.077415943146 - performance 0.9520\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(9346.077415943146, 0.9520333333333333)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "train.test_model(net, testloader, loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device, mask, layers_to_prune=[\"1\", \"4\", \"7\", \"10\"]):\n",
    "    for X, y in dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        y_hat = model(X)\n",
    "\n",
    "        loss = loss_fn(y_hat, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        ##### we must neutralize the gradient on the pruned params before the optimizer takes a step ####\n",
    "\n",
    "        if mask is not None:\n",
    "            for (name, param), m in zip(model.named_parameters(), mask):\n",
    "                if any([l in name for l in layers_to_prune]):\n",
    "                    param.grad *= m\n",
    "\n",
    "        ######\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = performance(y_hat, y)\n",
    "\n",
    "        loss_meter.update(val=loss.item(), n=X.shape[0])\n",
    "        performance_meter.update(val=acc, n=X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, loss_fn, optimizer, num_epochs, checkpoint_loc=None, checkpoint_name=\"checkpoint.pt\", performance=accuracy, lr_scheduler=None, device=None, mask=None, params_type_to_prune=[\"weight\", \"bias\"]):\n",
    "    if checkpoint_loc is not None:\n",
    "        os.makedirs(checkpoint_loc, exist_ok=True)\n",
    "\n",
    "    if device is None:\n",
    "        device = use_gpu_if_possible()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_meter = AverageMeter()\n",
    "        performance_meter = AverageMeter()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} --- learning rate {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "\n",
    "        train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device, mask)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum} - average: {loss_meter.avg}; Performance: {performance_meter.avg}\")\n",
    "\n",
    "        if checkpoint_name is not None and checkpoint_loc is not None:\n",
    "            checkpoint_dict = {\n",
    "                \"parameters\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            torch.save(checkpoint_dict, os.path.join(checkpoint_loc, checkpoint_name))\n",
    "        \n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    return loss_meter.sum, performance_meter.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 --- learning rate 0.01000\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'layers_to_prune' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2f596cfe7aaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-fc54ddc4ad35>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, loss_fn, optimizer, num_epochs, checkpoint_loc, checkpoint_name, performance, lr_scheduler, device, mask, params_type_to_prune)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1} --- learning rate {optimizer.param_groups[0]['lr']:.5f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_meter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperformance_meter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperformance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum} - average: {loss_meter.avg}; Performance: {performance_meter.avg}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-b77a27f92703>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device, mask)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers_to_prune\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layers_to_prune' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "lr = 0.01\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "\n",
    "train_model(net, trainloader, loss_fn, optimizer, num_epochs, device=\"cpu\", mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.test_model(net, testloader, loss_fn=loss_fn)"
   ]
  }
 ]
}