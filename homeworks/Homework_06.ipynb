{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "dssc_dl_2021",
   "display_name": "Python (DL virtualenv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Homework 6\n",
    "\n",
    "Taking inspiration from the last 2 pictures within the notebook (07-convnets.ipynb), implement a U-Net-style CNN with the following specs:\n",
    " \n",
    "1. All convolutions must use a 3 x 3 kernel and **leave the spatial dimensions (i.e. height, width) of the input untouched**.\n",
    "2. Downsampling in the contracting part is performed via maxpooling with a 2 x 2 kernel and stride of 2.\n",
    "3. Upsampling is operated by a deconvolution with a 2 x 2 kernel and stride of 2. The PyTorch module that implements the deconvolution is `nn.ConvTranspose2d`\n",
    "4. The final layer of the expanding part has only 1 channel\n",
    "    * between how many classes are we discriminating?\n",
    " \n",
    "Create a network class with (at least) a `__init__` and a `forward` method. Please resort to additional structures (e.g., `nn.Module`s, private methods...) if you believe it helps readability of your code.\n",
    " \n",
    "Test, at least with random data, that the network is doing the correct tensor operations and that the output has the correct shape (e.g., use `assert`s in your code to see if the byproduct is of the expected shape).\n",
    " \n",
    "Note: the overall organization of your work can greatly improve readability and understanding of your code by others. Please consider preparing your notebook in an organized fashion so that we can better understand (and correct) your implementation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![](../labs/img/unet_small.png)\n",
    "\n",
    "#### U-Net [3](https://arxiv.org/abs/1505.04597)\n",
    "\n",
    "The architecture is a composition of two parts: a contracting module and an expanding module.\n",
    "\n",
    "The contracting module is a sequence of VGG convolutional blocks. \n",
    "\n",
    "After reaching the bottom part, we begin upsampling the image following the inverse of the scheme from the contracting module, with an additional operation: we concatenate the output of the upsampling with the output from the last convolutional layer of the corresponding block (as in the image). Thus, if the upsampling yields a 256-channel output, we concatenate this output with the output of the last 256-channel convolutional layer from the contracting module. This leaves us with a 512-channel tensor which we convolve to 256-channels once again. Note that, if the spatial dimensions of the data from the contracting module doesn't match those of the upsampled data, cropping is operated so that we can safely concatenate the two tensors.\n",
    "\n",
    "Actually, the original implementation of U-Net operates a semantic segmentation on a window which is approximately 2/3 of the original image (there will hence be a leftover band of pixels outside the center of the image). In the image below, the white thin lines represent the area that will be subject to the segmentation.\n",
    "\n",
    "![](../labs/img/unet_crop.jpg)\n",
    "\n",
    "For what concerns the output, instead, we end up with a tensor of shape $h^\\prime \\times w^\\prime \\times C$, where $C$ denotes the number of the classes we want to operate segmentation (logically speaking, **if we want to classify each pixel, we wish to produce a softmax for each pixel**).\n",
    "\n",
    "![](../labs/img/unet_last.jpg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms as T\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Contraction(nn.Module):\n",
    "    def _build_vgg_block(self, num_conv_layers, in_channels, out_channels, kernel_size=3, batchnorm=True, activation=nn.ReLU, maxpool=True):\n",
    "        layers = []\n",
    "        for i in range(num_conv_layers):\n",
    "            if i == 0:\n",
    "                num_channels_in = in_channels\n",
    "            else:\n",
    "                num_channels_in = out_channels\n",
    "            \n",
    "            layers.append(nn.Conv2d(in_channels=num_channels_in, out_channels=out_channels, kernel_size=kernel_size, stride=2))\n",
    "            if batchnorm:\n",
    "                layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(activation())\n",
    "        \n",
    "        if maxpool:\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def __init__(self, num_classes=10, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            self._build_vgg_block(2, in_channels, 16, activation=nn.SiLU),\n",
    "            self._build_vgg_block(2, 16, 32, activation=nn.SiLU)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.classifier(self.avgpool(self.features(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expansion(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=64, activ=nn.ReLU, bias=False):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=7, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            activ(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes=10, in_channels=3, base_width=64, activ=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.prep = ResNetPrep(in_channels=in_channels, out_channels=base_width, activ=activ)\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            ResidualBlock(in_channels=base_width, n_channels=base_width, activ=activ),\n",
    "            ResidualBlock(in_channels=base_width, n_channels=base_width, activ=activ),\n",
    "\n",
    "            ResidualBlock(in_channels=base_width, n_channels=base_width * 2, activ=activ, downsample=True),\n",
    "            ResidualBlock(in_channels=base_width * 2, n_channels=base_width * 2, activ=activ),\n",
    "\n",
    "            ResidualBlock(in_channels=base_width * 2, n_channels=base_width * 4, activ=activ, downsample=True),\n",
    "            ResidualBlock(in_channels=base_width * 4, n_channels=base_width * 4, activ=activ),\n",
    "\n",
    "            ResidualBlock(in_channels=base_width * 4, n_channels=base_width * 8, activ=activ, downsample=True),\n",
    "            ResidualBlock(in_channels=base_width * 8, n_channels=base_width * 8, activ=activ)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.classifier = nn.Linear(in_features=base_width*8, out_features=num_classes, bias=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = self.prep(X)\n",
    "        out = self.res_blocks(out)\n",
    "        out = self.classifier(self.avgpool(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CNN(in_channels=3)\n",
    "_ = summary(net)"
   ]
  },
  {
   "source": [
    "Test if the net works on random data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((100,3,28,28))\n",
    "y = net(X)\n",
    "y.shape"
   ]
  },
  {
   "source": [
    "### References\n",
    "\n",
    "[1](https://arxiv.org/abs/1603.07285) Dumoulin, Vincent, and Francesco Visin. \"A guide to convolution arithmetic for deep learning.\"\n",
    "\n",
    "[2](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) He, Kaiming, et al. \"Deep residual learning for image recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition\n",
    "\n",
    "[3](https://arxiv.org/abs/1505.04597) Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" International Conference on Medical image computing and computer-assisted intervention.\n",
    "\n",
    "[4](https://arxiv.org/abs/1605.07146) Zagoruyko, Sergey, and Nikos Komodakis. \"Wide residual networks.\"\n",
    "\n",
    "[5](https://arxiv.org/abs/1409.1556v6) Simonyan and Zisserman. \"Very Deep Convolutional Networks for Large-Scale Image Recognition.\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}