{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "As always, please provide your solution in a Jupyter Notebook.\n",
    "\n",
    "1. Now that you have all the tools to train an MLP with high performance on MNIST, try reaching 0-loss (or 100% accuracy) on the training data (with a small epsilon, e.g. 99.99% training performance -- don't worry if you overfit!). The implementation is completely up to you. You just need to keep it an MLP without using fancy layers (e.g., keep the `Linear` layers, don't use `Conv1d` or something like this, don't use attention). You are free to use any LR scheduler or optimizer, any one of batchnorm/groupnorm, regularization methods... If you use something we haven't seen during lectures, please motivate your choice and explain (as briefly as possible) how it works.\n",
    "\n",
    "2. Try reaching 0-loss on the training data with **permuted labels**. Assess the model on the test data (without permuted labels) and comment. Help yourself with [3](https://arxiv.org/abs/1611.03530). *Tip:* To permute the labels, act on the `trainset.targets` with an appropriate torch function. Then, you can pass this \"permuted\" `Dataset` to a `DataLoader` like so:\n",
    "`trainloader_permuted = torch.utils.data.DataLoader(trainset_permuted, batch_size=batch_size_train, shuffle=True)`.\n",
    "You can now use this `DataLoader` inside the training function. Additional view for motivating this exercise: [\"The statistical significance perfect linear separation\", by Jared Tanner (Oxford U.)](https://www.youtube.com/watch?v=vl2QsVWEqdA).\n",
    "\n",
    "[3](https://arxiv.org/abs/1611.03530) Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2016). Understanding deep learning requires rethinking generalization.\n",
    "\n",
    "P.S. I increased the number of files to upload from 1 to 5, so if you want you may include up to 4 local image files in your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "\n",
    "\n",
    "sys.path.append(\"../labs\")\n",
    "\n",
    "from scripts import mnist\n",
    "from scripts.train_utils import accuracy, AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device):\n",
    "    for X, y in dataloader:\n",
    "        # TRANSFER X AND y TO GPU IF SPECIFIED\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # 1. reset the gradients previously accumulated by the optimizer\n",
    "        #    this will avoid re-using gradients from previous loops\n",
    "        optimizer.zero_grad() \n",
    "        # 2. get the predictions from the current state of the model\n",
    "        #    this is the forward pass\n",
    "        y_hat = model(X)\n",
    "        # 3. calculate the loss on the current mini-batch\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        # 4. execute the backward pass given the current loss\n",
    "        loss.backward()\n",
    "        # 5. update the value of the params\n",
    "        optimizer.step()\n",
    "        # 6. calculate the accuracy for this mini-batch\n",
    "        acc = performance(y_hat, y)\n",
    "        # 7. update the loss and accuracy AverageMeter\n",
    "        loss_meter.update(val=loss.item(), n=X.shape[0])\n",
    "        performance_meter.update(val=acc, n=X.shape[0])\n",
    "\n",
    "\n",
    "def train_model(model, dataloader, loss_fn, optimizer, num_epochs, checkpoint_loc=None, checkpoint_name=\"checkpoint.pt\", performance=accuracy, device=None):\n",
    "\n",
    "    # establish device\n",
    "    if device is None:\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Training on {device}\")\n",
    "\n",
    "    # create the folder for the checkpoints (if it's not None)\n",
    "    if checkpoint_loc is not None:\n",
    "        os.makedirs(checkpoint_loc, exist_ok=True)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # epoch loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # modify the hyperparameters of our optimizer mid-training\n",
    "        if epoch in (9, 19, 29):\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg[\"lr\"] *= .1\n",
    "        loss_meter = AverageMeter()\n",
    "        performance_meter = AverageMeter()\n",
    "\n",
    "        train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum} - average: {loss_meter.avg}; Performance: {performance_meter.avg}\")\n",
    "\n",
    "        # produce checkpoint dictionary -- but only if the name and folder of the checkpoint are not None\n",
    "        if checkpoint_name is not None and checkpoint_loc is not None:\n",
    "            checkpoint_dict = {\n",
    "                \"parameters\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            torch.save(checkpoint_dict, os.path.join(checkpoint_loc, checkpoint_name))\n",
    "\n",
    "    return loss_meter.sum, performance_meter.avg\n",
    "\n",
    "def test_model(model, dataloader, performance=accuracy, loss_fn=None, device=None):\n",
    "\n",
    "    # establish device\n",
    "    if device is None:\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Testing on {device}\")\n",
    "    \n",
    "    # create an AverageMeter for the loss if passed\n",
    "    if loss_fn is not None:\n",
    "        loss_meter = AverageMeter()\n",
    "    \n",
    "    performance_meter = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = model(X)\n",
    "            loss = loss_fn(y_hat, y) if loss_fn is not None else None\n",
    "            acc = performance(y_hat, y)\n",
    "            if loss_fn is not None:\n",
    "                loss_meter.update(loss.item(), X.shape[0])\n",
    "            performance_meter.update(acc, X.shape[0])\n",
    "    # get final performances\n",
    "    fin_loss = loss_meter.sum if loss_fn is not None else None\n",
    "    fin_perf = performance_meter.avg\n",
    "    print(f\"TESTING - loss {fin_loss if fin_loss is not None else '--'} - performance {fin_perf}\")\n",
    "    return fin_loss, fin_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_size_train = 256\n",
    "minibatch_size_test = 512\n",
    "\n",
    "trainloader, testloader, trainset, testset = mnist.get_data(data_root=\"../labs/datasets/\", batch_size_train=minibatch_size_test, batch_size_test=minibatch_size_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flat = torch.nn.Flatten()\n",
    "        self.layer1 = torch.nn.Linear(28*28, 512)\n",
    "        self.layer2 = torch.nn.Linear(512, 32)\n",
    "        self.layer3 = torch.nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, X): \n",
    "        out = self.flat(X)\n",
    "        out = self.layer1(out)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.layer2(out)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.layer3(out)\n",
    "        out = torch.nn.functional.log_softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the vanilla SGD with momentum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "lr = 0.01\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angela/Documenti/Deep Learning/DSSC_DL_2021/venv/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n",
      "/home/angela/Documenti/Deep Learning/DSSC_DL_2021/venv/lib/python3.6/site-packages/torch/autograd/__init__.py:147: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Loss - total: 21557.1648144722 - average: 0.3592860802412033; Performance: 0.8872\n",
      "Epoch 2 completed. Loss - total: 6554.022935390472 - average: 0.10923371558984121; Performance: 0.9670166666666666\n",
      "Epoch 3 completed. Loss - total: 5037.587438344955 - average: 0.08395979063908258; Performance: 0.9737833333333333\n",
      "Epoch 4 completed. Loss - total: 3570.4207960367203 - average: 0.05950701326727867; Performance: 0.9816833333333334\n",
      "Epoch 5 completed. Loss - total: 3343.7776812314987 - average: 0.05572962802052498; Performance: 0.9824666666666667\n",
      "Epoch 6 completed. Loss - total: 3149.705152928829 - average: 0.05249508588214715; Performance: 0.9827\n",
      "Epoch 7 completed. Loss - total: 2531.7850933670998 - average: 0.042196418222785; Performance: 0.98625\n",
      "Epoch 8 completed. Loss - total: 2497.6445816755295 - average: 0.04162740969459216; Performance: 0.98685\n",
      "Epoch 9 completed. Loss - total: 2213.0989110320807 - average: 0.036884981850534675; Performance: 0.9879833333333333\n",
      "Epoch 10 completed. Loss - total: 778.8975317329168 - average: 0.012981625528881947; Performance: 0.9960666666666667\n",
      "Epoch 11 completed. Loss - total: 375.52145083621144 - average: 0.0062586908472701906; Performance: 0.9983833333333333\n",
      "Epoch 12 completed. Loss - total: 269.50386859476566 - average: 0.004491731143246094; Performance: 0.99905\n",
      "Epoch 13 completed. Loss - total: 217.28782251290977 - average: 0.003621463708548496; Performance: 0.99935\n",
      "Epoch 14 completed. Loss - total: 175.6933977007866 - average: 0.00292822329501311; Performance: 0.9995\n",
      "Epoch 15 completed. Loss - total: 149.22882697358727 - average: 0.0024871471162264546; Performance: 0.9996166666666667\n",
      "Epoch 16 completed. Loss - total: 128.09296331880614 - average: 0.0021348827219801023; Performance: 0.9996833333333334\n",
      "Epoch 17 completed. Loss - total: 111.76804497651756 - average: 0.001862800749608626; Performance: 0.99975\n",
      "Epoch 18 completed. Loss - total: 96.5052805878222 - average: 0.0016084213431303699; Performance: 0.9997833333333334\n",
      "Epoch 19 completed. Loss - total: 85.07826472073793 - average: 0.0014179710786789655; Performance: 0.9998166666666667\n",
      "Epoch 20 completed. Loss - total: 70.52837041812018 - average: 0.001175472840302003; Performance: 0.99985\n",
      "Epoch 21 completed. Loss - total: 68.5018449369818 - average: 0.0011416974156163633; Performance: 0.9998666666666667\n",
      "Epoch 22 completed. Loss - total: 67.07008182816207 - average: 0.0011178346971360345; Performance: 0.9998833333333333\n",
      "Epoch 23 completed. Loss - total: 66.05035359598696 - average: 0.0011008392265997827; Performance: 0.9998833333333333\n",
      "Epoch 24 completed. Loss - total: 64.95960101485252 - average: 0.0010826600169142088; Performance: 0.9999\n",
      "Epoch 25 completed. Loss - total: 64.00078624300659 - average: 0.0010666797707167765; Performance: 0.9999\n",
      "Epoch 26 completed. Loss - total: 62.884708074852824 - average: 0.0010480784679142137; Performance: 0.9999\n",
      "Epoch 27 completed. Loss - total: 61.77990036364645 - average: 0.0010296650060607742; Performance: 0.9998833333333333\n",
      "Epoch 28 completed. Loss - total: 60.73517438955605 - average: 0.0010122529064926008; Performance: 0.9999166666666667\n",
      "Epoch 29 completed. Loss - total: 59.74452926684171 - average: 0.0009957421544473618; Performance: 0.9999\n",
      "Epoch 30 completed. Loss - total: 58.02163189742714 - average: 0.0009670271982904524; Performance: 0.9999166666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(58.02163189742714, 0.9999166666666667)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model, trainloader, loss_fn, optimizer, num_epochs, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angela/Documenti/Deep Learning/DSSC_DL_2021/venv/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING - loss 57.83349530771375 - performance 0.9999166666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(57.83349530771375, 0.9999166666666667)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model, testloader, performance=accuracy, loss_fn=loss_fn, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we managed to reach a preformance of $99.99\\%$ and an average loss of $~0.000967$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init_function(proc_id):\n",
    "    process_seed = torch.initial_seed()\n",
    "    # Back out the base_seed so we can use all the bits.\n",
    "    base_seed = process_seed - proc_id\n",
    "    ss = np.random.SeedSequence([proc_id, base_seed])\n",
    "    # More than 128 bits (4 32-bit words) would be overkill.\n",
    "    np.random.seed(ss.generate_state(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = torch.randperm(len(trainset.targets))\n",
    "trainset_permuted = copy.deepcopy(trainset)\n",
    "trainset_permuted.targets = trainset.targets[perm]\n",
    "trainloader_permuted = torch.utils.data.DataLoader(trainset_permuted, batch_size=minibatch_size_train, shuffle=True, num_workers=4, worker_init_fn=worker_init_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flat = torch.nn.Flatten()\n",
    "        self.layer1 = torch.nn.Linear(28*28, 1024)\n",
    "        self.layer2 = torch.nn.Linear(1024, 512)\n",
    "        self.layer3 = torch.nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, X): \n",
    "        out = self.flat(X)\n",
    "        out = self.layer1(out)\n",
    "        out = torch.nn.functional.leaky_relu(out)\n",
    "        out = self.layer2(out)\n",
    "        out = torch.nn.functional.leaky_relu(out)\n",
    "        out = self.layer3(out)\n",
    "        out = torch.nn.functional.log_softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "lr = 0.001\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angela/Documenti/Deep Learning/DSSC_DL_2021/venv/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n",
      "/home/angela/Documenti/Deep Learning/DSSC_DL_2021/venv/lib/python3.6/site-packages/torch/autograd/__init__.py:147: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Loss - total: 138250.0248184204 - average: 2.304167080307007; Performance: 0.10598333333333333\n",
      "Epoch 2 completed. Loss - total: 138130.28281402588 - average: 2.3021713802337644; Performance: 0.11103333333333333\n",
      "Epoch 3 completed. Loss - total: 138090.6477279663 - average: 2.301510795466105; Performance: 0.11113333333333333\n",
      "Epoch 4 completed. Loss - total: 138029.12187194824 - average: 2.3004853645324705; Performance: 0.11375\n",
      "Epoch 5 completed. Loss - total: 137932.97441101074 - average: 2.298882906850179; Performance: 0.1162\n",
      "Epoch 6 completed. Loss - total: 137807.49898529053 - average: 2.296791649754842; Performance: 0.1173\n",
      "Epoch 7 completed. Loss - total: 137628.49787902832 - average: 2.2938082979838055; Performance: 0.1211\n",
      "Epoch 8 completed. Loss - total: 137274.79403686523 - average: 2.2879132339477537; Performance: 0.12405\n",
      "Epoch 9 completed. Loss - total: 136771.8848953247 - average: 2.2795314149220784; Performance: 0.13353333333333334\n",
      "Epoch 10 completed. Loss - total: 134933.01887512207 - average: 2.248883647918701; Performance: 0.15333333333333332\n",
      "Epoch 11 completed. Loss - total: 133853.65522003174 - average: 2.230894253667196; Performance: 0.1615\n",
      "Epoch 12 completed. Loss - total: 133064.5390777588 - average: 2.2177423179626463; Performance: 0.17018333333333333\n",
      "Epoch 13 completed. Loss - total: 132348.28787994385 - average: 2.2058047979990643; Performance: 0.17696666666666666\n",
      "Epoch 14 completed. Loss - total: 131593.09237670898 - average: 2.193218206278483; Performance: 0.18491666666666667\n",
      "Epoch 15 completed. Loss - total: 130830.58296966553 - average: 2.180509716161092; Performance: 0.19178333333333333\n",
      "Epoch 16 completed. Loss - total: 130031.1388168335 - average: 2.167185646947225; Performance: 0.19953333333333334\n",
      "Epoch 17 completed. Loss - total: 129200.08518218994 - average: 2.153334753036499; Performance: 0.2062\n",
      "Epoch 18 completed. Loss - total: 128339.38410949707 - average: 2.1389897351582845; Performance: 0.2153\n",
      "Epoch 19 completed. Loss - total: 127410.8611907959 - average: 2.1235143531799316; Performance: 0.22321666666666667\n",
      "Epoch 20 completed. Loss - total: 125969.58417510986 - average: 2.0994930695851646; Performance: 0.23856666666666668\n",
      "Epoch 21 completed. Loss - total: 125746.67631530762 - average: 2.0957779385884603; Performance: 0.23996666666666666\n",
      "Epoch 22 completed. Loss - total: 125606.29956817627 - average: 2.0934383261362712; Performance: 0.24143333333333333\n",
      "Epoch 23 completed. Loss - total: 125488.03712463379 - average: 2.0914672854105634; Performance: 0.24191666666666667\n",
      "Epoch 24 completed. Loss - total: 125374.2954788208 - average: 2.08957159131368; Performance: 0.24313333333333334\n",
      "Epoch 25 completed. Loss - total: 125267.90980529785 - average: 2.087798496754964; Performance: 0.24378333333333332\n",
      "Epoch 26 completed. Loss - total: 125165.78578948975 - average: 2.086096429824829; Performance: 0.24455\n",
      "Epoch 27 completed. Loss - total: 125063.95561981201 - average: 2.0843992603302004; Performance: 0.2459\n",
      "Epoch 28 completed. Loss - total: 124963.40747070312 - average: 2.082723457845052; Performance: 0.24628333333333333\n",
      "Epoch 29 completed. Loss - total: 124865.55954742432 - average: 2.0810926591237386; Performance: 0.24765\n",
      "Epoch 30 completed. Loss - total: 124708.69303894043 - average: 2.078478217315674; Performance: 0.24935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(124708.69303894043, 0.24935)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model, trainloader_permuted, loss_fn, optimizer, num_epochs, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angela/Documenti/Deep Learning/DSSC_DL_2021/venv/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING - loss 145964.36348724365 - performance 0.11561666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(145964.36348724365, 0.11561666666666667)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model, testloader, performance=accuracy, loss_fn=loss_fn, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the performance is $10\\%$ makes me think that the model is always predicting the same class, thus being right $1/10$ of the times (since we have $10$ classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f33e6d0ae48>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(trainset.data[15].numpy(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f33e54adbe0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(trainset_permuted.data[15].numpy(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angela/Documenti/Deep Learning/DSSC_DL_2021/venv/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 10]) torch.Size([256])\n",
      "234\n"
     ]
    }
   ],
   "source": [
    "for i, (X, y) in enumerate(trainloader_permuted):\n",
    "    if i == 15:\n",
    "        y_hat = model(X)\n",
    "        print(y_hat.shape, y.shape)\n",
    "        \n",
    "print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL virtualenv)",
   "language": "python",
   "name": "dssc_dl_2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
