{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "dssc_dl_2021",
   "display_name": "Python (DL virtualenv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Homework 4\n",
    "\n",
    "As always, please provide your solution in a Jupyter Notebook.\n",
    "\n",
    "1. Now that you have all the tools to train an MLP with high performance on MNIST, try reaching 0-loss (or 100% accuracy) on the training data (with a small epsilon, e.g. 99.99% training performance -- don't worry if you overfit!). The implementation is completely up to you. You just need to keep it an MLP without using fancy layers (e.g., keep the `Linear` layers, don't use `Conv1d` or something like this, don't use attention). You are free to use any LR scheduler or optimizer, any one of batchnorm/groupnorm, regularization methods... If you use something we haven't seen during lectures, please motivate your choice and explain (as briefly as possible) how it works.\n",
    "\n",
    "2. Try reaching 0-loss on the training data with **permuted labels**. Assess the model on the test data (without permuted labels) and comment. Help yourself with [3](https://arxiv.org/abs/1611.03530). *Tip:* To permute the labels, act on the `trainset.targets` with an appropriate torch function. Then, you can pass this \"permuted\" `Dataset` to a `DataLoader` like so:\n",
    "`trainloader_permuted = torch.utils.data.DataLoader(trainset_permuted, batch_size=batch_size_train, shuffle=True)`.\n",
    "You can now use this `DataLoader` inside the training function. Additional view for motivating this exercise: [link](https://youtu.be/vl2QsVWEqdA)\n",
    "\n",
    "P.S. I increased the number of files to upload from 1 to 5, so if you want you may include up to 4 local image files in your solution."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "sys.path.append(\"../labs\")\n",
    "\n",
    "from scripts import mnist\n",
    "from scripts.train_utils import accuracy, AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flat = torch.nn.Flatten()\n",
    "        self.layer1 = torch.nn.Linear(28*28, 16)\n",
    "        self.layer2 = torch.nn.Linear(16, 32)\n",
    "        self.layer3 = torch.nn.Linear(32, 24)\n",
    "        self.layer4 = torch.nn.Linear(24, 10)\n",
    "\n",
    "    def forward(self, X): \n",
    "        out = self.flat(X)\n",
    "        out = self.layer1(out)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.layer2(out)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.layer3(out)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.layer4(out)\n",
    "        out = torch.nn.functional.log_softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device):\n",
    "    for X, y in dataloader:\n",
    "        # TRANSFER X AND y TO GPU IF SPECIFIED\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # 1. reset the gradients previously accumulated by the optimizer\n",
    "        #    this will avoid re-using gradients from previous loops\n",
    "        optimizer.zero_grad() \n",
    "        # 2. get the predictions from the current state of the model\n",
    "        #    this is the forward pass\n",
    "        y_hat = model(X)\n",
    "        # 3. calculate the loss on the current mini-batch\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        # 4. execute the backward pass given the current loss\n",
    "        loss.backward()\n",
    "        # 5. update the value of the params\n",
    "        optimizer.step()\n",
    "        # 6. calculate the accuracy for this mini-batch\n",
    "        acc = performance(y_hat, y)\n",
    "        # 7. update the loss and accuracy AverageMeter\n",
    "        loss_meter.update(val=loss.item(), n=X.shape[0])\n",
    "        performance_meter.update(val=acc, n=X.shape[0])\n",
    "\n",
    "\n",
    "def train_model(model, dataloader, loss_fn, optimizer, num_epochs, checkpoint_loc=None, checkpoint_name=\"checkpoint.pt\", performance=accuracy, device=None):\n",
    "\n",
    "    # establish device\n",
    "    if device is None:\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Training on {device}\")\n",
    "\n",
    "    # create the folder for the checkpoints (if it's not None)\n",
    "    if checkpoint_loc is not None:\n",
    "        os.makedirs(checkpoint_loc, exist_ok=True)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # epoch loop\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_meter = AverageMeter()\n",
    "        performance_meter = AverageMeter()\n",
    "\n",
    "        train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum} - average: {loss_meter.avg}; Performance: {performance_meter.avg}\")\n",
    "\n",
    "        # produce checkpoint dictionary -- but only if the name and folder of the checkpoint are not None\n",
    "        if checkpoint_name is not None and checkpoint_loc is not None:\n",
    "            checkpoint_dict = {\n",
    "                \"parameters\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            torch.save(checkpoint_dict, os.path.join(checkpoint_loc, checkpoint_name))\n",
    "\n",
    "    return loss_meter.sum, performance_meter.avg\n",
    "\n",
    "def test_model(model, dataloader, performance=accuracy, loss_fn=None, device=None):\n",
    "\n",
    "    # establish device\n",
    "    if device is None:\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Testing on {device}\")\n",
    "    \n",
    "    # create an AverageMeter for the loss if passed\n",
    "    if loss_fn is not None:\n",
    "        loss_meter = AverageMeter()\n",
    "    \n",
    "    performance_meter = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = model(X)\n",
    "            loss = loss_fn(y_hat, y) if loss_fn is not None else None\n",
    "            acc = performance(y_hat, y)\n",
    "            if loss_fn is not None:\n",
    "                loss_meter.update(loss.item(), X.shape[0])\n",
    "            performance_meter.update(acc, X.shape[0])\n",
    "    # get final performances\n",
    "    fin_loss = loss_meter.sum if loss_fn is not None else None\n",
    "    fin_perf = performance_meter.avg\n",
    "    print(f\"TESTING - loss {fin_loss if fin_loss is not None else '--'} - performance {fin_perf}\")\n",
    "    return fin_loss, fin_perf"
   ]
  },
  {
   "source": [
    "Let's use the vanilla SGD with momentum:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .1\n",
    "wd = 5e-4\n",
    "momentum = .9\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=wd, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_size_train = 256\n",
    "minibatch_size_test = 512\n",
    "\n",
    "trainloader, testloader, trainset, testset = mnist.get_data(data_root=\"../labs/datasets/\", batch_size_train=minibatch_size_test, batch_size_test=minibatch_size_test)"
   ]
  },
  {
   "source": [
    "Let's train our network:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training on cpu\n",
      "/home/angela/Documenti/Deep Learning/DSSC_DL_2021/venv/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "Epoch 1 completed. Loss - total: 17731.810479164124 - average: 0.2955301746527354; Performance: 0.9110333333333334\n",
      "Epoch 2 completed. Loss - total: 12130.750637054443 - average: 0.20217917728424073; Performance: 0.9382333333333334\n",
      "Epoch 3 completed. Loss - total: 10631.62873840332 - average: 0.17719381230672201; Performance: 0.9460166666666666\n",
      "Epoch 4 completed. Loss - total: 9510.347358226776 - average: 0.1585057893037796; Performance: 0.9510833333333333\n",
      "Epoch 5 completed. Loss - total: 9027.319495916367 - average: 0.15045532493193944; Performance: 0.95385\n",
      "Epoch 6 completed. Loss - total: 8393.216874599457 - average: 0.13988694790999096; Performance: 0.9570833333333333\n",
      "Epoch 7 completed. Loss - total: 7721.327761888504 - average: 0.12868879603147507; Performance: 0.9597333333333333\n",
      "Epoch 8 completed. Loss - total: 7570.34238743782 - average: 0.12617237312396368; Performance: 0.9607\n",
      "Epoch 9 completed. Loss - total: 7577.6309159994125 - average: 0.1262938485999902; Performance: 0.96135\n",
      "Epoch 10 completed. Loss - total: 7227.217956542969 - average: 0.12045363260904948; Performance: 0.9623333333333334\n",
      "Epoch 11 completed. Loss - total: 7153.996017456055 - average: 0.1192332669576009; Performance: 0.9627\n",
      "Epoch 12 completed. Loss - total: 7251.228248119354 - average: 0.12085380413532257; Performance: 0.96155\n",
      "Epoch 13 completed. Loss - total: 6368.5805678367615 - average: 0.10614300946394603; Performance: 0.96745\n",
      "Epoch 14 completed. Loss - total: 6445.305614948273 - average: 0.10742176024913788; Performance: 0.96605\n",
      "Epoch 15 completed. Loss - total: 6545.200106620789 - average: 0.10908666844367981; Performance: 0.9656833333333333\n",
      "Epoch 16 completed. Loss - total: 6372.4150767326355 - average: 0.10620691794554392; Performance: 0.96605\n",
      "Epoch 17 completed. Loss - total: 6458.0643256902695 - average: 0.10763440542817115; Performance: 0.9666166666666667\n",
      "Epoch 18 completed. Loss - total: 6204.482051372528 - average: 0.10340803418954213; Performance: 0.96755\n",
      "Epoch 19 completed. Loss - total: 6173.8209228515625 - average: 0.10289701538085938; Performance: 0.9671333333333333\n",
      "Epoch 20 completed. Loss - total: 6076.459994792938 - average: 0.10127433324654897; Performance: 0.9679\n",
      "Epoch 21 completed. Loss - total: 6075.80383014679 - average: 0.10126339716911316; Performance: 0.9674833333333334\n",
      "Epoch 22 completed. Loss - total: 5792.089000701904 - average: 0.09653481667836507; Performance: 0.9691833333333333\n",
      "Epoch 23 completed. Loss - total: 6174.554523468018 - average: 0.10290924205780029; Performance: 0.96665\n",
      "Epoch 24 completed. Loss - total: 5584.535940170288 - average: 0.09307559900283814; Performance: 0.9702833333333334\n",
      "Epoch 25 completed. Loss - total: 5779.127913236618 - average: 0.09631879855394364; Performance: 0.9691\n",
      "Epoch 26 completed. Loss - total: 5754.725855469704 - average: 0.09591209759116173; Performance: 0.9691\n",
      "Epoch 27 completed. Loss - total: 5576.13453745842 - average: 0.092935575624307; Performance: 0.9703166666666667\n",
      "Epoch 28 completed. Loss - total: 5622.568344593048 - average: 0.09370947240988413; Performance: 0.9695833333333334\n",
      "Epoch 29 completed. Loss - total: 5786.673017978668 - average: 0.09644455029964447; Performance: 0.96935\n",
      "Epoch 30 completed. Loss - total: 5874.961523771286 - average: 0.0979160253961881; Performance: 0.9680666666666666\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5874.961523771286, 0.9680666666666666)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "train_model(model, trainloader, loss_fn, optimizer, num_epochs, device=\"cpu\")"
   ]
  },
  {
   "source": [
    "And now let's test it:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing on cpu\n",
      "/home/angela/Documenti/Deep Learning/DSSC_DL_2021/venv/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "TESTING - loss -- - performance 0.96785\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(None, 0.96785)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "test_model(model, testloader, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing on cpu\n",
      "/home/angela/Documenti/Deep Learning/DSSC_DL_2021/venv/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "TESTING - loss 5965.80991601944 - performance 0.96785\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5965.80991601944, 0.96785)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "test_model(model, testloader, performance=accuracy, loss_fn=loss_fn, device=\"cpu\")"
   ]
  }
 ]
}