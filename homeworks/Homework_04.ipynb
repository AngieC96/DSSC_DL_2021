{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "As always, please provide your solution in a Jupyter Notebook.\n",
    "\n",
    "1. Now that you have all the tools to train an MLP with high performance on MNIST, try reaching 0-loss (or 100% accuracy) on the training data (with a small epsilon, e.g. 99.99% training performance -- don't worry if you overfit!). The implementation is completely up to you. You just need to keep it an MLP without using fancy layers (e.g., keep the `Linear` layers, don't use `Conv1d` or something like this, don't use attention). You are free to use any LR scheduler or optimizer, any one of batchnorm/groupnorm, regularization methods... If you use something we haven't seen during lectures, please motivate your choice and explain (as briefly as possible) how it works.\n",
    "\n",
    "2. Try reaching 0-loss on the training data with **permuted labels**. Assess the model on the test data (without permuted labels) and comment. Help yourself with [3](https://arxiv.org/abs/1611.03530). *Tip:* To permute the labels, act on the `trainset.targets` with an appropriate torch function. Then, you can pass this \"permuted\" `Dataset` to a `DataLoader` like so:\n",
    "`trainloader_permuted = torch.utils.data.DataLoader(trainset_permuted, batch_size=batch_size_train, shuffle=True)`.\n",
    "You can now use this `DataLoader` inside the training function. Additional view for motivating this exercise: [\"The statistical significance perfect linear separation\", by Jared Tanner (Oxford U.)](https://www.youtube.com/watch?v=vl2QsVWEqdA).\n",
    "\n",
    "[3](https://arxiv.org/abs/1611.03530) Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2016). Understanding deep learning requires rethinking generalization.\n",
    "\n",
    "P.S. I increased the number of files to upload from 1 to 5, so if you want you may include up to 4 local image files in your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "\n",
    "\n",
    "sys.path.append(\"../labs\")\n",
    "\n",
    "from scripts import mnist\n",
    "from scripts.train_utils import accuracy, AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device):\n",
    "    for X, y in dataloader:\n",
    "        # TRANSFER X AND y TO GPU IF SPECIFIED\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # 1. reset the gradients previously accumulated by the optimizer\n",
    "        #    this will avoid re-using gradients from previous loops\n",
    "        optimizer.zero_grad() \n",
    "        # 2. get the predictions from the current state of the model\n",
    "        #    this is the forward pass\n",
    "        y_hat = model(X)\n",
    "        # 3. calculate the loss on the current mini-batch\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        # 4. execute the backward pass given the current loss\n",
    "        loss.backward()\n",
    "        # 5. update the value of the params\n",
    "        optimizer.step()\n",
    "        # 6. calculate the accuracy for this mini-batch\n",
    "        acc = performance(y_hat, y)\n",
    "        # 7. update the loss and accuracy AverageMeter\n",
    "        loss_meter.update(val=loss.item(), n=X.shape[0])\n",
    "        performance_meter.update(val=acc, n=X.shape[0])\n",
    "\n",
    "\n",
    "def train_model(model, dataloader, loss_fn, optimizer, num_epochs, checkpoint_loc=None, checkpoint_name=\"checkpoint.pt\", performance=accuracy, device=None):\n",
    "\n",
    "    # establish device\n",
    "    if device is None:\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Training on {device}\")\n",
    "\n",
    "    # create the folder for the checkpoints (if it's not None)\n",
    "    if checkpoint_loc is not None:\n",
    "        os.makedirs(checkpoint_loc, exist_ok=True)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # epoch loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # modify the hyperparameters of our optimizer mid-training\n",
    "        if epoch in (9, 19, 29):\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg[\"lr\"] *= .1\n",
    "        loss_meter = AverageMeter()\n",
    "        performance_meter = AverageMeter()\n",
    "\n",
    "        train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum} - average: {loss_meter.avg}; Performance: {performance_meter.avg}\")\n",
    "\n",
    "        # produce checkpoint dictionary -- but only if the name and folder of the checkpoint are not None\n",
    "        if checkpoint_name is not None and checkpoint_loc is not None:\n",
    "            checkpoint_dict = {\n",
    "                \"parameters\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            torch.save(checkpoint_dict, os.path.join(checkpoint_loc, checkpoint_name))\n",
    "\n",
    "    return loss_meter.sum, performance_meter.avg\n",
    "\n",
    "def test_model(model, dataloader, performance=accuracy, loss_fn=None, device=None):\n",
    "\n",
    "    # establish device\n",
    "    if device is None:\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Testing on {device}\")\n",
    "    \n",
    "    # create an AverageMeter for the loss if passed\n",
    "    if loss_fn is not None:\n",
    "        loss_meter = AverageMeter()\n",
    "    \n",
    "    performance_meter = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = model(X)\n",
    "            loss = loss_fn(y_hat, y) if loss_fn is not None else None\n",
    "            acc = performance(y_hat, y)\n",
    "            if loss_fn is not None:\n",
    "                loss_meter.update(loss.item(), X.shape[0])\n",
    "            performance_meter.update(acc, X.shape[0])\n",
    "    # get final performances\n",
    "    fin_loss = loss_meter.sum if loss_fn is not None else None\n",
    "    fin_perf = performance_meter.avg\n",
    "    print(f\"TESTING - loss {fin_loss if fin_loss is not None else '--'} - performance {fin_perf}\")\n",
    "    return fin_loss, fin_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flat = torch.nn.Flatten()\n",
    "        self.layer1 = torch.nn.Linear(28*28, 512)\n",
    "        self.layer2 = torch.nn.Linear(512, 32)\n",
    "        self.layer3 = torch.nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, X): \n",
    "        out = self.flat(X)\n",
    "        out = self.layer1(out)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.layer2(out)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.layer3(out)\n",
    "        out = torch.nn.functional.log_softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_size_train = 256\n",
    "minibatch_size_test = 512\n",
    "\n",
    "trainloader, testloader, trainset, testset = mnist.get_data(data_root=\"../labs/datasets/\", batch_size_train=minibatch_size_test, batch_size_test=minibatch_size_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the vanilla SGD with momentum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "num_epochs = 30\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angela/Documenti/Deep Learning/DSSC_DL_2021/venv/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n",
      "/home/angela/Documenti/Deep Learning/DSSC_DL_2021/venv/lib/python3.6/site-packages/torch/autograd/__init__.py:147: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Loss - total: 21557.1648144722 - average: 0.3592860802412033; Performance: 0.8872\n",
      "Epoch 2 completed. Loss - total: 6554.022935390472 - average: 0.10923371558984121; Performance: 0.9670166666666666\n",
      "Epoch 3 completed. Loss - total: 5037.587438344955 - average: 0.08395979063908258; Performance: 0.9737833333333333\n",
      "Epoch 4 completed. Loss - total: 3570.4207960367203 - average: 0.05950701326727867; Performance: 0.9816833333333334\n",
      "Epoch 5 completed. Loss - total: 3343.7776812314987 - average: 0.05572962802052498; Performance: 0.9824666666666667\n",
      "Epoch 6 completed. Loss - total: 3149.705152928829 - average: 0.05249508588214715; Performance: 0.9827\n",
      "Epoch 7 completed. Loss - total: 2531.7850933670998 - average: 0.042196418222785; Performance: 0.98625\n",
      "Epoch 8 completed. Loss - total: 2497.6445816755295 - average: 0.04162740969459216; Performance: 0.98685\n",
      "Epoch 9 completed. Loss - total: 2213.0989110320807 - average: 0.036884981850534675; Performance: 0.9879833333333333\n",
      "Epoch 10 completed. Loss - total: 778.8975317329168 - average: 0.012981625528881947; Performance: 0.9960666666666667\n",
      "Epoch 11 completed. Loss - total: 375.52145083621144 - average: 0.0062586908472701906; Performance: 0.9983833333333333\n",
      "Epoch 12 completed. Loss - total: 269.50386859476566 - average: 0.004491731143246094; Performance: 0.99905\n",
      "Epoch 13 completed. Loss - total: 217.28782251290977 - average: 0.003621463708548496; Performance: 0.99935\n",
      "Epoch 14 completed. Loss - total: 175.6933977007866 - average: 0.00292822329501311; Performance: 0.9995\n",
      "Epoch 15 completed. Loss - total: 149.22882697358727 - average: 0.0024871471162264546; Performance: 0.9996166666666667\n",
      "Epoch 16 completed. Loss - total: 128.09296331880614 - average: 0.0021348827219801023; Performance: 0.9996833333333334\n",
      "Epoch 17 completed. Loss - total: 111.76804497651756 - average: 0.001862800749608626; Performance: 0.99975\n",
      "Epoch 18 completed. Loss - total: 96.5052805878222 - average: 0.0016084213431303699; Performance: 0.9997833333333334\n",
      "Epoch 19 completed. Loss - total: 85.07826472073793 - average: 0.0014179710786789655; Performance: 0.9998166666666667\n",
      "Epoch 20 completed. Loss - total: 70.52837041812018 - average: 0.001175472840302003; Performance: 0.99985\n",
      "Epoch 21 completed. Loss - total: 68.5018449369818 - average: 0.0011416974156163633; Performance: 0.9998666666666667\n",
      "Epoch 22 completed. Loss - total: 67.07008182816207 - average: 0.0011178346971360345; Performance: 0.9998833333333333\n",
      "Epoch 23 completed. Loss - total: 66.05035359598696 - average: 0.0011008392265997827; Performance: 0.9998833333333333\n",
      "Epoch 24 completed. Loss - total: 64.95960101485252 - average: 0.0010826600169142088; Performance: 0.9999\n",
      "Epoch 25 completed. Loss - total: 64.00078624300659 - average: 0.0010666797707167765; Performance: 0.9999\n",
      "Epoch 26 completed. Loss - total: 62.884708074852824 - average: 0.0010480784679142137; Performance: 0.9999\n",
      "Epoch 27 completed. Loss - total: 61.77990036364645 - average: 0.0010296650060607742; Performance: 0.9998833333333333\n",
      "Epoch 28 completed. Loss - total: 60.73517438955605 - average: 0.0010122529064926008; Performance: 0.9999166666666667\n",
      "Epoch 29 completed. Loss - total: 59.74452926684171 - average: 0.0009957421544473618; Performance: 0.9999\n",
      "Epoch 30 completed. Loss - total: 58.02163189742714 - average: 0.0009670271982904524; Performance: 0.9999166666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(58.02163189742714, 0.9999166666666667)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model, trainloader, loss_fn, optimizer, num_epochs, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angela/Documenti/Deep Learning/DSSC_DL_2021/venv/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING - loss 57.83349530771375 - performance 0.9999166666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(57.83349530771375, 0.9999166666666667)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model, testloader, performance=accuracy, loss_fn=loss_fn, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we managed to reach a preformance of $99.99\\%$ and an average loss of $~0.000967$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init_function(proc_id):\n",
    "    process_seed = torch.initial_seed()\n",
    "    # Back out the base_seed so we can use all the bits.\n",
    "    base_seed = process_seed - proc_id\n",
    "    ss = np.random.SeedSequence([proc_id, base_seed])\n",
    "    # More than 128 bits (4 32-bit words) would be overkill.\n",
    "    np.random.seed(ss.generate_state(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = torch.randperm(len(trainset.targets))\n",
    "trainset_permuted = copy.deepcopy(trainset)\n",
    "trainset_permuted.targets = trainset.targets[perm]\n",
    "trainloader_permuted = torch.utils.data.DataLoader(trainset_permuted, batch_size=minibatch_size_train, shuffle=True, num_workers=4, worker_init_fn=worker_init_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flat = torch.nn.Flatten()\n",
    "        self.layer1 = torch.nn.Linear(28*28, 1024)\n",
    "        self.layer2 = torch.nn.Linear(1024, 512)\n",
    "        self.layer3 = torch.nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, X): \n",
    "        out = self.flat(X)\n",
    "        out = self.layer1(out)\n",
    "        out = torch.nn.functional.leaky_relu(out)\n",
    "        out = self.layer2(out)\n",
    "        out = torch.nn.functional.leaky_relu(out)\n",
    "        out = self.layer3(out)\n",
    "        out = torch.nn.functional.log_softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angela/Documenti/Deep Learning/DSSC_DL_2021/venv/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Loss - total: 138238.08685302734 - average: 2.303968114217122; Performance: 0.1082\n",
      "Epoch 2 completed. Loss - total: 138117.38721466064 - average: 2.3019564535776773; Performance: 0.10971666666666667\n",
      "Epoch 3 completed. Loss - total: 138081.0179824829 - average: 2.3013502997080484; Performance: 0.11105\n",
      "Epoch 4 completed. Loss - total: 138039.53478240967 - average: 2.300658913040161; Performance: 0.11346666666666666\n",
      "Epoch 5 completed. Loss - total: 137972.44120025635 - average: 2.299540686670939; Performance: 0.11356666666666666\n",
      "Epoch 6 completed. Loss - total: 137877.99146270752 - average: 2.2979665243784586; Performance: 0.1171\n",
      "Epoch 7 completed. Loss - total: 137694.70840454102 - average: 2.29491180674235; Performance: 0.11811666666666666\n",
      "Epoch 8 completed. Loss - total: 137440.06665039062 - average: 2.2906677775065103; Performance: 0.12111666666666666\n",
      "Epoch 9 completed. Loss - total: 137072.2717666626 - average: 2.28453786277771; Performance: 0.12615\n",
      "Epoch 10 completed. Loss - total: 135595.37591552734 - average: 2.259922931925456; Performance: 0.14153333333333334\n",
      "Epoch 11 completed. Loss - total: 134761.07093048096 - average: 2.2460178488413494; Performance: 0.15001666666666666\n",
      "Epoch 12 completed. Loss - total: 134150.38480377197 - average: 2.235839746729533; Performance: 0.1566\n",
      "Epoch 13 completed. Loss - total: 133543.90047454834 - average: 2.2257316745758056; Performance: 0.16323333333333334\n",
      "Epoch 14 completed. Loss - total: 132918.75229644775 - average: 2.215312538274129; Performance: 0.1696\n",
      "Epoch 15 completed. Loss - total: 132262.12901306152 - average: 2.204368816884359; Performance: 0.17668333333333333\n",
      "Epoch 16 completed. Loss - total: 131567.4720840454 - average: 2.1927912014007567; Performance: 0.18441666666666667\n",
      "Epoch 17 completed. Loss - total: 130842.18277740479 - average: 2.1807030462900796; Performance: 0.19076666666666667\n",
      "Epoch 18 completed. Loss - total: 130068.5576248169 - average: 2.167809293746948; Performance: 0.19735\n",
      "Epoch 19 completed. Loss - total: 129236.07827758789 - average: 2.1539346379597983; Performance: 0.2064\n",
      "Epoch 20 completed. Loss - total: 127949.88996887207 - average: 2.132498166147868; Performance: 0.22096666666666667\n",
      "Epoch 21 completed. Loss - total: 127740.05849456787 - average: 2.1290009749094647; Performance: 0.22236666666666666\n",
      "Epoch 22 completed. Loss - total: 127608.255859375 - average: 2.126804264322917; Performance: 0.22303333333333333\n",
      "Epoch 23 completed. Loss - total: 127496.16536712646 - average: 2.124936089452108; Performance: 0.22416666666666665\n",
      "Epoch 24 completed. Loss - total: 127395.15316009521 - average: 2.1232525526682537; Performance: 0.22471666666666668\n",
      "Epoch 25 completed. Loss - total: 127296.70420074463 - average: 2.121611736679077; Performance: 0.22551666666666667\n",
      "Epoch 26 completed. Loss - total: 127202.16454315186 - average: 2.1200360757191974; Performance: 0.22626666666666667\n",
      "Epoch 27 completed. Loss - total: 127108.90657806396 - average: 2.118481776301066; Performance: 0.22698333333333334\n",
      "Epoch 28 completed. Loss - total: 127017.13007354736 - average: 2.116952167892456; Performance: 0.22875\n",
      "Epoch 29 completed. Loss - total: 126928.94696044922 - average: 2.11548244934082; Performance: 0.22901666666666667\n",
      "Epoch 30 completed. Loss - total: 126784.41877746582 - average: 2.113073646291097; Performance: 0.2303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(126784.41877746582, 0.2303)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model, trainloader_permuted, loss_fn, optimizer, num_epochs, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angela/Documenti/Deep Learning/DSSC_DL_2021/venv/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING - loss 147942.67664337158 - performance 0.10771666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(147942.67664337158, 0.10771666666666667)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model, testloader, performance=accuracy, loss_fn=loss_fn, device=\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL virtualenv)",
   "language": "python",
   "name": "dssc_dl_2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
