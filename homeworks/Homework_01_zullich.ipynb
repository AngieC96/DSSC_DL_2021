{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38664bitlotteryce31c0a482d54873875cc8eb2d66ce61",
   "display_name": "Python 3.8.6 64-bit ('lottery')",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "These are the \"official\" result for this HW session.\n",
    "As I cited multiple times during lab, Python and PyTorch offer multiple solutions to many tasks, hence things may be done differently than I show here."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchsummary"
   ]
  },
  {
   "source": [
    "**Ex. 1**: implement the neural network from the figure. No bias terms, relu activation for hidden layers, softmax for output."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # connections from input layer leading to first layer\n",
    "        self.layer1 = torch.nn.Linear(in_features=5, out_features=11, bias=False)\n",
    "        self.layer2 = torch.nn.Linear(in_features=11, out_features=16, bias=False)\n",
    "        self.layer3 = torch.nn.Linear(in_features=16, out_features=13, bias=False)\n",
    "        self.layer4 = torch.nn.Linear(in_features=13, out_features=8, bias=False)\n",
    "        # connections leading to output layer\n",
    "        self.layer5 = torch.nn.Linear(in_features=8, out_features=4, bias=False)\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.layer1(X)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.layer2(out)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.layer3(out)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.layer4(out)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.layer5(out)\n",
    "        out = torch.nn.functional.softmax(out, dim=1) # note: works also without specifying the dim, but raises a deprecation warning\n",
    "        return out"
   ]
  },
  {
   "source": [
    "**Ex. 2**: print a summary of the net"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\n├─Linear: 1-1                            55\n├─Linear: 1-2                            176\n├─Linear: 1-3                            208\n├─Linear: 1-4                            104\n├─Linear: 1-5                            32\n=================================================================\nTotal params: 575\nTrainable params: 575\nNon-trainable params: 0\n=================================================================\n"
     ]
    }
   ],
   "source": [
    "net = MLP()\n",
    "_ = torchsummary.summary(net) # note: \"_ =\" avoids the double printing of the output"
   ]
  },
  {
   "source": [
    "**Ex. 3:** calculation of the no. of parameter for both the biasless and the not biasless case\n",
    "\n",
    "a) The biasless case is straightforward: for each layer (except the input), we multiply the no. of neurons for the no. of neurons of the previous layer. This gives\n",
    "\n",
    "$11*5+16*11+13*16+8*13+4*8 = 575$ as in the output above\n",
    "\n",
    "b) The not-biasless case is also straightforward given a): we just need to add the number of neurons to each of the layers with incoming connections (i.e., except the input), as there is one bias term per neuron with incoming connection (*note, also the output layer since it has incoming connections!*)\n",
    "\n",
    "$575 + 11+16+13+8+4 = 627$\n",
    "\n",
    "Also, can be done like so:\n",
    "\n",
    "$(\\text{no. neurons}_{l-1} + 1) \\cdot \\text{no. neurons}_l,~~l\\in\\{1,\\dots 5\\}$ (here the layer 0 is the input layer of course)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Ex. 4**: printing norms of the parameters\n",
    "\n",
    "This exercise had multiple solutions depending on the type of norm used.\n",
    "\n",
    "The parameters are organized in matrices, so with L-1 and L-2 norm there can be some confusion about whether to use:\n",
    "\n",
    "- vector norms for \"unrolled\"/\"flattened\" matrices (`torch.norm`)\n",
    "- matrix 1-norm and 2-norm (`torch.linalg.norm`)\n",
    "\n",
    "Nonetheless, both solutions are good since the goal of the exercise was to put together the analysis of the `state_dict` with some easy lineal algebra capabilities of PT."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Param name \t L1 norm \t L2 norm\nlayer1.weight tensor(13.3416) tensor(2.0372)\nlayer1.weight tensor(3.2201) tensor(1.3089)\nlayer2.weight tensor(25.5253) tensor(2.2320)\nlayer2.weight tensor(2.8155) tensor(1.0845)\nlayer3.weight tensor(25.6307) tensor(2.0681)\nlayer3.weight tensor(2.2152) tensor(0.9858)\nlayer4.weight tensor(14.5009) tensor(1.6472)\nlayer4.weight tensor(1.3541) tensor(0.9130)\nlayer5.weight tensor(5.3351) tensor(1.0939)\nlayer5.weight tensor(0.9057) tensor(0.8626)\n"
     ]
    }
   ],
   "source": [
    "print(\"Param name\", \"\\t\", \"L1 norm\", \"\\t\", \"L2 norm\")\n",
    "for param_name, param in net.state_dict().items(): # also good: \"for param_name, param in net.named_parameters()\" OR \"for param in parameters()\"...\n",
    "    print(param_name, param.norm(1), param.norm(2)) # this returns the vector L1-L2-norm\n",
    "    print(param_name, torch.linalg.norm(param, 1), torch.linalg.norm(param, 2)) # this return the matricial 1- and 2-norm -- good as well\n",
    "    # note: if you use torch.linalg.norm(param), this returns the Frobenius matrix norm which is equivalent to the vector L2-norm"
   ]
  },
  {
   "source": [
    "Note: if you wish to get rid of the singleton tensor, just append `.item()`.\n",
    "Don't do something like `.detach().numpy()` because you're basically wasting computation in converting the tensor to numpy, then to scalar."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}