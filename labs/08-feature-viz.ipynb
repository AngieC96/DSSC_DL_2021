{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Deep Learning course - LAB 8\n",
    "\n",
    "## Feature Visualization in ConvNets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Recap from previous Lab\n",
    "\n",
    "* we learned to build some popular Convolutional Neural Network (CNN) architectyres\n",
    "* we saw how to implement various techniques of Transfer Learning on CNNs\n",
    "\n",
    "### Agenda for today\n",
    "\n",
    "* we will learn about two main staples of Feature Visualization (FV) in CNNs:\n",
    "    * saliency maps\n",
    "    * Deep Dream"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Feature Visualization\n",
    "\n",
    "FV is one of the main tools of Explainable AI (XAI) in CNNs. The aim is to answer the following question: «*What are the (visual) features that are being looked at/searched for by the model for a given image or class?*»\n",
    "\n",
    "The field of research is wide and a lot of work has been done on this field, yet, there is still so much to learn and the framework is all but unified. The main reference for the works in this field are contained in the e-journal [distill.pub](distill.pub), and namely the articles by Christopher Olah and his group @ OpenAI, who concentrate on analyzing what single neurons or convolutional filters learn in specific CNN architectures and weights configuration. It's a daunting task because you have to scrutinize single neurons or filters in a huge CNN and analyze response to 1+ million images. The results, though, are staggering, and really let us draw considerations on the capability of the network, especially considering the aspect of **generalization**.\n",
    "\n",
    "Below, a representation of a neuron in a ResNet50 trained on ImageNet, built as an «artificial, optimized image that maximizes activations of the given unit.»\n",
    "\n",
    "![](img/trump_neurons.jpg)\n",
    "\n",
    "Image from [OpenAI Microscope](https://microscope.openai.com/models/contrastive_4x/image_block_4_5_Add_6_0/89).\n",
    "\n",
    "Note: in the same layer, there are 2559 more neurons to analyze..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Saliency Maps\n",
    "\n",
    "Saliency Maps (SMs) are one of the first example of FV for CNNs. The motivation behind them is to search which pixel of a given image was more *salient* in the classification of an image in a given class $c\\in \\{1,\\dots,C\\}$.\n",
    "\n",
    "To rephrase it, we wish to find an approximation of which part of an image was deemed more important by the CNN relative to the given class $c$.\n",
    "\n",
    "The intuition behind it is simple: pick an image $I_0$, forward-propagate it through the CNN, then backpropagate the gradient **on the image itself** (NB: we backpropagate on the pixels, not on the weights).\n",
    "\n",
    "Let us call $M\\in\\mathbb{R}^{h\\times w}$ the SM, and $M_{ij}$ the map for pixel $i, j$.\n",
    "\n",
    "In the case of a grayscale image $I_0\\in\\mathbb{R}^{h\\times w}$, the SM is:\n",
    "\n",
    "$M_{ij} = \\vert \\partial \\text{score} / \\partial I_0  \\vert$\n",
    "\n",
    "where $\\text{score}$ is the prediction of the model.\n",
    "\n",
    "If we have a multi-channel (e.g. color) image $I_0\\in\\mathbb{R}^{h\\times w\\times \\text{ch}}$, than the SM is:\n",
    "\n",
    "$M_{ij} = \\max_{\\text{ch}}\\{\\vert \\partial \\text{score} / \\partial I_0  \\vert\\}$\n",
    "\n",
    "We may also have **negative saliency maps** and **positive saliency maps**:\n",
    "\n",
    "$M_{ij}^{(+)} = \\max\\{M_{ij}, 0\\};~~M_{ij}^{(-)} = - \\min\\{M_{ij}, 0\\}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}