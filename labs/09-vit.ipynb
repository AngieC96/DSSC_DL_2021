{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd02e6347a50883dfa0598d3f478411c8d6a5b9cf8792810af1a6fbd779ad8b1967",
   "display_name": "Python 3.8.8 64-bit ('lot': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "2e6347a50883dfa0598d3f478411c8d6a5b9cf8792810af1a6fbd779ad8b1967"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Deep Learning course - LAB 9\n",
    "\n",
    "## An explainability-first implementation of the Vision Transformer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This lab will mainly follow the slides from the lecture on the Vision Transformer (ViT).\n",
    "\n",
    "Please refer to the slides for the methodological explanations.\n",
    "\n",
    "We will be constructing the ViT bottom-up, i.e. from the input embedding to the output."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary"
   ]
  },
  {
   "source": [
    "## 1a. Patch + vectorize input\n",
    "\n",
    "The input is first subdivided into patches and each patch is *unrolled* into a 1D vector.\n",
    "\n",
    "Let us implement a generic torchvision-style transform which we can pass to a `Dataset`'s `transform` attribute."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToVecPatch():\n",
    "    def __init__(self, image_size, patch_size):\n",
    "        if isinstance(image_size, int):\n",
    "            image_size = (image_size, image_size)\n",
    "        if isinstance(patch_size, int):\n",
    "            patch_size = (patch_size, patch_size)\n",
    "        assert image_size[0] % patch_size[0] == 0 and image_size[1] % patch_size[1] == 0, \\\n",
    "            f\"The image size should be a multiple of the patch size. Found {image_size} and {patch_size}.\"\n",
    "        self.patch_size = patch_size\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        '''\n",
    "        sample is a torch.Tensor\n",
    "        '''\n",
    "        patch_vert = torch.cat(sample.split(self.patch_size[1], dim=1))\n",
    "        patch_horiz = torch.stack(patch_vert.split(self.patch_size[0], dim=0))\n",
    "        vec_patches = torch.flatten(patch_horiz, start_dim=1)\n",
    "        return vec_patches"
   ]
  },
  {
   "source": [
    "Let's see it in action on a small 4x4 grayscale image"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.,  4.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 7.,  8.,  9.,  0.],\n",
       "        [10.,  7.,  3., 88.]])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "x = torch.Tensor([[1,2,3,4],[4,5,6,7],[7,8,9,0],[10,7,3,88]])\n",
    "patch_size = 2\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.],\n",
       "        [ 4.,  5.],\n",
       "        [ 7.,  8.],\n",
       "        [10.,  7.],\n",
       "        [ 3.,  4.],\n",
       "        [ 6.,  7.],\n",
       "        [ 9.,  0.],\n",
       "        [ 3., 88.]])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "patch_vert = torch.cat(x.split(patch_size, dim=1))\n",
    "patch_vert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  2.],\n",
       "         [ 4.,  5.]],\n",
       "\n",
       "        [[ 7.,  8.],\n",
       "         [10.,  7.]],\n",
       "\n",
       "        [[ 3.,  4.],\n",
       "         [ 6.,  7.]],\n",
       "\n",
       "        [[ 9.,  0.],\n",
       "         [ 3., 88.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "patch_horiz = torch.stack(patch_vert.split(patch_size, dim=0))\n",
    "patch_horiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  4.,  5.],\n",
       "        [ 7.,  8., 10.,  7.],\n",
       "        [ 3.,  4.,  6.,  7.],\n",
       "        [ 9.,  0.,  3., 88.]])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "vec_patches = torch.flatten(patch_horiz, start_dim=1)\n",
    "vec_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  4.,  5.],\n",
       "        [ 7.,  8., 10.,  7.],\n",
       "        [ 3.,  4.,  6.,  7.],\n",
       "        [ 9.,  0.,  3., 88.]])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "P = ToVecPatch(4, 2)\n",
    "P(sample=x)"
   ]
  },
  {
   "source": [
    "# 1b. Input embedding\n",
    "\n",
    "Now we need to take care of the input embedding:\n",
    "* we have an input $I$ with shape $N \\times P^2\\cdot c$, where:\n",
    "    * $N$ is the number of patches\n",
    "    * $P$ is the patch size\n",
    "    * $c$ is the channel size (1 in the example above)\n",
    "* we need to linearly project $I$ into $z_0$, belonging in the $N \\times D$ space, where $D$ is (hopefully) smaller than $P^2\\cdot c$\n",
    "* we also need to prepend a learnable `<class>` token to $z_0$\n",
    "* and we need to sum the **positional embedding/encoding** to it\n",
    "\n",
    "Also, in a `Module`-like class, we need to take into account that the input will be 3-dimensional ($B \\times N \\times P^2\\cdot c$, $B$ being the batch size)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedInput(nn.Module):\n",
    "    def __init__(self, num_patches, patch_dim, latent_dim, bias=False, dropout_p=0.0):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Linear(patch_dim, latent_dim, bias=bias) # this represents the matrix E\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        # the next params are the same independent of the batch size\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, latent_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, latent_dim))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        z = self.embed(X)\n",
    "        z = self.dropout(z)\n",
    "        z = torch.cat((self.class_token.expand(z.shape[0], *self.class_token.shape[1:]), z), dim=1)\n",
    "        z += self.pos_embedding\n",
    "        return z"
   ]
  },
  {
   "source": [
    "## 2. Attention\n",
    "\n",
    "* We have an embedded input $z_0$ of shape $B\\times (N+1)\\times D$\n",
    "\n",
    "* We need to:\n",
    "    * get $Q, K, V \\in \\mathbb{R}^{B\\times (N+1)\\times d}$ through linear projection from $z_0$\n",
    "    * obtain $A = \\text{softmax}(QK^\\top/\\sqrt{d})$\n",
    "    * get $S = AV$\n",
    "\n",
    "all this for each head $h\\in\\{1,\\dots H\\}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadedSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads, input_dim, attention_dim, bias=False, dropout_p=0.0):\n",
    "        '''\n",
    "        input_dim -> D\n",
    "        attention_dim -> d\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dim = attention_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.u_qkv = nn.Linear(input_dim, attention_dim * num_heads * 3, bias=bias)\n",
    "        self.u_msa = nn.Linear(attention_dim * num_heads, input_dim, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        QKV = self.u_qkv(z).chunks(3, dim=-1)\n",
    "        separate_heads = lambda tensor: tensor.reshape(*tensor.shape[:2], self.num_heads, self.attention_dim).permute(0,2,1,3)\n",
    "        Q, K, V = [separate_heads(t) for t in QKV]\n",
    "        '''\n",
    "        Why all that mess?\n",
    "            Out of the linear projection we get a tensor of shape B x (N+1) x 3Hd\n",
    "            We separate this tensor into three chunks of shape B x (N+1) x Hd\n",
    "            We now need to \"enucleate\" the head from the third dim (->reshape)\n",
    "            Then, for simplicity, we shift the head to the second dim (->permute)\n",
    "            Shape: B x H x (N+1) x d\n",
    "            Now, for each head, we need to do the dot product between Q and K\n",
    "            This can be done in an elegant way using the einstein notation (einsum)\n",
    "        '''\n",
    "        A = torch.einsum(\"b h n d, b h m d -> b h n m\", Q, K) / (self.attention_dim ** .5)\n",
    "        '''\n",
    "        We can use only small letters (no capitals)\n",
    "        b is batch size, h is head size, d is attention_dim\n",
    "        n and m are the no. of patches for Q and K respectively\n",
    "        Despite being =, we must name them differently so torch knows\n",
    "        how to carry out the product\n",
    "        '''\n",
    "        A = torch.nn.functional.softmax(A, dim=-1)\n",
    "        S = torch.einsum(\"b h n m, b h m d -> b h n d\", A, V)\n",
    "        # undo separate_heads\n",
    "        S = S.permute(0, 2, 1, 3)\n",
    "        S = S.reshape(*S.shape[:2], S.shape[2]*S.shape[3])\n",
    "        S = u_msa(S)\n",
    "        return self.dropout(S)\n",
    "        "
   ]
  },
  {
   "source": [
    "## 3. MLP layer\n",
    "\n",
    "Very easy, let's do it by ourselves...\n",
    "\n",
    "$(B\\times (N+1) \\times D) \\rightarrow (B\\times (N+1)\\times m) \\rightarrow (B\\times (N+1)\\times D)$ \n",
    "\n",
    "**add dropout wherever it's possible**\n",
    "\n",
    "**use `GeLU` non-linearity**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, bias=True, dropout_p=0.0):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        pass\n"
   ]
  },
  {
   "source": [
    "## 4. The MSA Layer\n",
    "\n",
    "We need to put together 2. and 3.\n",
    "\n",
    "![](img/msa_layer.jpg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSALayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, attention_dim, mlp_dim, bias_msa=False, bias_mlp=True):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.msa = MultiheadedSelfAttention(num_heads, embed_dim, attention_dim, bias=bias_msa)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, mlp_dim, bias=bias_mlp)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # DIY\n",
    "        pass"
   ]
  },
  {
   "source": [
    "## 5. The final MLP head\n",
    "\n",
    "Easy...\n",
    "\n",
    "$(B \\times D) \\rightarrow (B \\times \\kappa)$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, bias=True):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes, bias=bias)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.fc(X)"
   ]
  },
  {
   "source": [
    "## Let's put all of our pieces together"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_patches,\n",
    "        patch_dim,\n",
    "        embed_dim,\n",
    "        num_msa_layers,\n",
    "        num_heads,\n",
    "        attention_dim,\n",
    "        mlp_dim,\n",
    "        num_classes,\n",
    "        bias_embed=False,\n",
    "        bias_msa=False,\n",
    "        bias_mlp_att=True,\n",
    "        bias_mlp_head=True\n",
    "        # no dropout for simplicity\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_embedder = EmbedInput(num_patches, patch_dim, embed_dim, bias=bias_embed)\n",
    "        self.msa = nn.Sequential(\n",
    "            *([MSALayer(embed_dim, num_heads, attention_dim, mlp_dim, bias_msa=bias_msa, bias_mlp=bias_mlp_att)] * num_msa_layers)\n",
    "        )\n",
    "        self.head = MLPHead(embed_dim, num_classes, bias=bias_mlp_head)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        X is already a tensor B images decomposed into vectorized patches\n",
    "        '''\n",
    "        out = self.input_embedder(X)\n",
    "        out = self.msa(out)\n",
    "        return self.head(out)\n"
   ]
  },
  {
   "source": [
    "### Instantiate a ViT-Base model\n",
    "\n",
    "![](img/vit_models.jpg)\n",
    "\n",
    "Build it for images of size 224x224 and patches of size 16x16 (→196 patches).\n",
    "\n",
    "We comply with the paper and set $d=D/H=768/12=64$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (input_embedder): EmbedInput(\n",
       "    (embed): Linear(in_features=256, out_features=768, bias=False)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (msa): Sequential(\n",
       "    (0): MSALayer(\n",
       "      (layernorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadedSelfAttention(\n",
       "        (u_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (u_msa): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (layernorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (1): MSALayer(\n",
       "      (layernorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadedSelfAttention(\n",
       "        (u_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (u_msa): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (layernorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (2): MSALayer(\n",
       "      (layernorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadedSelfAttention(\n",
       "        (u_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (u_msa): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (layernorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (3): MSALayer(\n",
       "      (layernorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadedSelfAttention(\n",
       "        (u_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (u_msa): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (layernorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (4): MSALayer(\n",
       "      (layernorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadedSelfAttention(\n",
       "        (u_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (u_msa): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (layernorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (5): MSALayer(\n",
       "      (layernorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadedSelfAttention(\n",
       "        (u_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (u_msa): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (layernorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (6): MSALayer(\n",
       "      (layernorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadedSelfAttention(\n",
       "        (u_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (u_msa): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (layernorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (7): MSALayer(\n",
       "      (layernorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadedSelfAttention(\n",
       "        (u_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (u_msa): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (layernorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (8): MSALayer(\n",
       "      (layernorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadedSelfAttention(\n",
       "        (u_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (u_msa): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (layernorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (9): MSALayer(\n",
       "      (layernorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadedSelfAttention(\n",
       "        (u_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (u_msa): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (layernorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (10): MSALayer(\n",
       "      (layernorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadedSelfAttention(\n",
       "        (u_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (u_msa): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (layernorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (11): MSALayer(\n",
       "      (layernorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (msa): MultiheadedSelfAttention(\n",
       "        (u_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (u_msa): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (layernorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "  )\n",
       "  (head): MLPHead(\n",
       "    (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "vit = ViT(num_patches=196, patch_dim=16*16, embed_dim=768, num_msa_layers=12, num_heads=12, attention_dim=64, mlp_dim=3072, num_classes=1000)\n",
    "vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================================================\nLayer (type:depth-idx)                        Param #\n======================================================================\n├─EmbedInput: 1-1                             --\n|    └─Linear: 2-1                            196,608\n|    └─Dropout: 2-2                           --\n├─Sequential: 1-2                             --\n|    └─MSALayer: 2-3                          --\n|    |    └─LayerNorm: 3-1                    1,536\n|    |    └─MultiheadedSelfAttention: 3-2     2,359,296\n|    |    └─LayerNorm: 3-3                    1,536\n|    |    └─MLP: 3-4                          --\n├─MLPHead: 1-3                                --\n|    └─Linear: 2-4                            769,000\n======================================================================\nTotal params: 3,327,976\nTrainable params: 3,327,976\nNon-trainable params: 0\n======================================================================\n"
     ]
    }
   ],
   "source": [
    "_ = summary(vit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "85340160"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "(768*3092*24) + (2359296+1536+1536)*12"
   ]
  },
  {
   "source": [
    "This was just a demo showcasing one of the possible ways we can construct a structure like the Visual Transformers.\n",
    "\n",
    "If you need to use it, I suggest using pre-built stuff, like the one contained in `timm`.\n",
    "\n",
    "You'll notice that existing implementations tend to make more use of the `einops` library, which introduces some methods, ubiquitous to PyTorch and NumPy, for transposing (permuting) a tensor, repeating given dims... Check out the [docs](https://einops.rocks/1-einops-basics/) if you're interested."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}