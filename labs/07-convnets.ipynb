{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38664bitlotteryce31c0a482d54873875cc8eb2d66ce61",
   "display_name": "Python 3.8.6 64-bit ('lottery': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "8aec2e4cca6a43ecda9b11f31ea0f9f4b012d28e6de8cbdf64a5e136ca9a5fb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Deep Learning course - LAB 7\n",
    "\n",
    "## ConvNets 101"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Recap from previous Lab\n",
    "\n",
    "* we introduced some basic image processing functionalities with OpenCV\n",
    "* we saw how to import a custom dataset in PyTorch, how to operate data augmentation, and how to create a DataLoader out of it\n",
    "\n",
    "### Agenda for today\n",
    "\n",
    "* we will construct our first Convolutional Neural Network (CNN)\n",
    "* we will show how to do transfer learning on CNNs\n",
    "* we will show how to introduce Deconvolution/Inverse Convolution inside CNNs\n",
    "* we will construct our own implementation of two famous CNN architectures: ResNet and U-Net"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Our first CNN\n",
    "\n",
    "Building CNNs is actually not hard once you know all the pieces to construct a MultiLayer Perceptron.\n",
    "\n",
    "We can distinguish between two macro-categories of CNNs, at least for the part concerning image classification. We might call them \"historical\" and \"modern\", although characteristics of both can sometimes get pretty much mixed-up.\n",
    "\n",
    "* \"Historical\" CNNs are a stack composed of two parts:\n",
    "  * a **convolutional** part, where have a cascade of convolutional layers intertwined with pooling layers for dimensionality and complexity reduction\n",
    "    * usually the filters in each convolutional layer are more numerous as the image size shrinks (i.e., as we get further from the input)\n",
    "  * a **fully-connected** part, where we have a sequence of few fully-connected layer, ending up in the output layer, where, as usual, we have as many neurons as there are categories\n",
    "  \n",
    " the epitome of the historical CNN (which is still used in research today nonetheless) is VGGNet (or simply VGG). Its *core* is a **convolutional block** composed of two or three convolutional layers each with the same number of filters, which is double the number of filters of the previous layer, up to 512 filters per layer. At the end of each block, there's a Max Pooling layer which halves the spatial dimension of the image.\n",
    "\n",
    " In the picture below, you can see a modern implementation of VGG with only one fully-connected layer.\n",
    "  ![](img/vgg.png)\n",
    "  \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* \"Modern\" CNNs, instead, get rid of the fully-connected part, as usually\n",
    "    1. it introduces a considerable amount of parameters in the network (note that the convolutional layers, due to their local connectivity and shared weights, have a much lower number of parameters w.r.t. fully connected layers)\n",
    "    2. it keeps the 2D spatial structure of the image intact up to the last hidden layer, allowing for more interpretal parameters/neurons (*insert Olah citation here*)\n",
    "    3. it represents a \"rigid\" portion of the network in the sense that it constrains they size of the image to be fixed. We will see later how this can pose a problem.\n",
    "\n",
    " Sometimes, even the pooling layers may get replaced by convolutional layers with large kernel size, as its effect is to reduce dimensionality (i.e., height and width) of the corresponding image.\n",
    " \n",
    " Recently, the **residual block** has become one of the paramount structures in modern CNNs.\n",
    "\n",
    " \"Classical\" CNNs stack a large number of convolutional layers on top of each other. This though can lead to problems such as the **vanishing gradient**. In a periodic fashion, resnets introduce **skip connections**:\n",
    "    * given the layer of index $k$, we call $a_k$ its output/activation\n",
    "    * $a_k$ gets pushed towards the next layer $k+1$, but we also \"propagate\" it *after* the layer $k+m$ (usually $m$ is 1 or 2) and we sum $a_k$ to $a_{k+m}$.\n",
    "    \n",
    " More on that later.\n",
    "\n",
    " ![](img/resnet.png)\n",
    "\n",
    " In the diagram above there're two residual blocks. The skip connections are indicated with the protruding connector."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models import vgg11_bn\n",
    "from torchsummary import summary\n",
    "from scripts import mnistm\n",
    "from scripts import mnist\n",
    "from scripts import train\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def _build_vgg_block(self, num_conv_layers, in_channels, out_channels, kernel_size=3, batchnorm=True, activation=nn.ReLU, maxpool=True):\n",
    "        layers = []\n",
    "        for i in range(num_conv_layers):\n",
    "            if i == 0:\n",
    "                num_channels_in = in_channels\n",
    "            else:\n",
    "                num_channels_in = out_channels\n",
    "            \n",
    "            layers.append(nn.Conv2d(in_channels=num_channels_in, out_channels=out_channels, kernel_size=kernel_size))\n",
    "            if batchnorm:\n",
    "                layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(activation())\n",
    "        \n",
    "        if maxpool:\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def __init__(self, num_classes=10, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            self._build_vgg_block(2, in_channels, 16, activation=nn.SiLU),\n",
    "            self._build_vgg_block(2, 16, 32, activation=nn.SiLU)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.classifier(self.avgpool(self.features(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\n├─Sequential: 1-1                        --\n|    └─Sequential: 2-1                   --\n|    |    └─Conv2d: 3-1                  448\n|    |    └─BatchNorm2d: 3-2             32\n|    |    └─SiLU: 3-3                    --\n|    |    └─Conv2d: 3-4                  2,320\n|    |    └─BatchNorm2d: 3-5             32\n|    |    └─SiLU: 3-6                    --\n|    |    └─MaxPool2d: 3-7               --\n|    └─Sequential: 2-2                   --\n|    |    └─Conv2d: 3-8                  4,640\n|    |    └─BatchNorm2d: 3-9             64\n|    |    └─SiLU: 3-10                   --\n|    |    └─Conv2d: 3-11                 9,248\n|    |    └─BatchNorm2d: 3-12            64\n|    |    └─SiLU: 3-13                   --\n|    |    └─MaxPool2d: 3-14              --\n├─AdaptiveAvgPool2d: 1-2                 --\n├─Sequential: 1-3                        --\n|    └─Flatten: 2-3                      --\n|    └─Linear: 2-4                       330\n=================================================================\nTotal params: 17,178\nTrainable params: 17,178\nNon-trainable params: 0\n=================================================================\n"
     ]
    }
   ],
   "source": [
    "net = CNN(in_channels=3)\n",
    "_ = summary(net)"
   ]
  },
  {
   "source": [
    "Test if the net works on random data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([100, 10])"
      ]
     },
     "metadata": {},
     "execution_count": 103
    }
   ],
   "source": [
    "X = torch.rand((100,3,28,28))\n",
    "y = net(X)\n",
    "y.shape"
   ]
  },
  {
   "source": [
    "Let's go more in detail in the computation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "in torch.Size([100, 3, 28, 28])\n0 torch.Size([100, 16, 12, 12])\n1 torch.Size([100, 32, 4, 4])\navgpool torch.Size([100, 32, 1, 1])\nout torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "z = X\n",
    "print(\"in\", z.shape)\n",
    "for i, mod in enumerate(net.features):\n",
    "    z = mod(z)\n",
    "    print(i, z.shape)\n",
    "\n",
    "z = net.avgpool(z)\n",
    "print(\"avgpool\", z.shape)\n",
    "\n",
    "z = net.classifier(z)\n",
    "print(\"out\", z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.4639, 0.4676, 0.4199], [0.2534, 0.2380, 0.2618]), # I pre-computed these data\n",
    "])\n",
    "\n",
    "mnistm_train = mnistm.MNISTM(root=\"datasets/MNISTM\", download=True, transform=transforms)\n",
    "mnistm_test = mnistm.MNISTM(root=\"datasets/MNISTM\", train=False, download=True, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ground truth: 5\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-13T16:18:46.462413</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 251.565 248.518125 \nL 251.565 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \nL 244.365 7.2 \nL 26.925 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pf8fe4910f2)\">\n    <image height=\"218\" id=\"image65ee9a7e24\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\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\" y=\"-6.64\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m1a46cbbd78\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#m1a46cbbd78\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#m1a46cbbd78\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#m1a46cbbd78\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#m1a46cbbd78\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#m1a46cbbd78\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#m1a46cbbd78\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m9b015c8337\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m9b015c8337\" y=\"11.082857\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m9b015c8337\" y=\"49.911429\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5 -->\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m9b015c8337\" y=\"88.74\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m9b015c8337\" y=\"127.568571\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 15 -->\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m9b015c8337\" y=\"166.397143\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m9b015c8337\" y=\"205.225714\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 25 -->\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 224.64 \nL 26.925 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 244.365 224.64 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 7.2 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pf8fe4910f2\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "img = (mnistm_train.train_data[0].numpy() * 255).astype(\"uint8\")\n",
    "plt.imshow(img)\n",
    "print(\"Ground truth:\", mnistm_train.train_labels[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([10000, 28, 28, 3]), torch.Size([10000]))"
      ]
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "source": [
    "mnistm_test.test_data.shape, mnistm_test.test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train = 128\n",
    "batch_test = 512\n",
    "mnistm_trainloader = DataLoader(mnistm_train, batch_train, shuffle=True, num_workers=8)\n",
    "mnistm_testloader = DataLoader(mnistm_test, batch_test, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(net.parameters(), lr=0.1, weight_decay=5e-4, momentum=.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 --- learning rate 0.10000\n",
      "Epoch 1 completed. Loss - total: 41027.25116109848 - average: 0.6837875193516413; Performance: 0.7757833333333334\n",
      "Epoch 2 --- learning rate 0.10000\n",
      "Exception ignored in: <function _releaseLock at 0x7fb408277820>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zullich/.pyenv/versions/3.8.6/lib/python3.8/logging/__init__.py\", line 223, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-de01cbbe4fcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnistm_trainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/zullich/LabControlli1/dl-course-21/labs/scripts/train.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, loss_fn, optimizer, num_epochs, checkpoint_loc, checkpoint_name, performance, lr_scheduler, device, lr_scheduler_step_on_epoch)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mlr_scheduler_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlr_scheduler_step_on_epoch\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_meter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperformance_meter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperformance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum} - average: {loss_meter.avg}; Performance: {performance_meter.avg}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/zullich/LabControlli1/dl-course-21/labs/scripts/train.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device, lr_scheduler)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_meter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperformance_meter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperformance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# note: I've added a generic performance to replace accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lottery/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lottery/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lottery/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lottery/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/lib/python3.8/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train.train_model(net, mnistm_trainloader, loss_fn=nn.CrossEntropyLoss(), optimizer=optim, num_epochs=50, lr_scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"models_push/cnn_mnistm/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"models_push/cnn_mnistm/model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TESTING - loss -- - performance 0.979\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(None, 0.979)"
      ]
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "train.test_model(net, mnistm_testloader)"
   ]
  },
  {
   "source": [
    "### Transfer Learning\n",
    "\n",
    "Transfer Learning is the utilization of a model, built for a given task (**upstream task**) on one or more different tasks (**downstream task**).\n",
    "\n",
    "Usually, in Deep Learning we operate a **model finetuning**, i.e., we use a model fully-trained on the upstream task and we adapt it someway on the downstream task.\n",
    "\n",
    "Fine-tuning comes in different flavors:\n",
    "\n",
    "1. we might just re-use the model for the upstream task without re-training\n",
    "\n",
    "2. we might want to re-train the model on the downstream task leaving fixed the parameters of the majority of the layers, while re-training only the remaining parameters.\n",
    " Usually, we train only the weights in the final fully-connected layer. This method is called **feature extraction**.\n",
    "\n",
    "3. we might want to fully re-train the model, using the `state_dict` from the upstream task as an **initialization** for the training on the downstream task. This is the proper **fine-tuning**.\n",
    "\n",
    "NB: if the ground truth for the downstream task and the upstream task have different domain (e.g. different number of categories), I'll need to modify the architecture of my CNN by **changing the number of output features** of the last layer. This, of course, will mean that the weights in this layer need to be re-initialized, hence, we'll for sure need to resort to strategies 2 or 3.\n",
    "\n",
    "NB2: a common techinque which is operated for **small downstream tasks datasets** is to obtain the features before classification (or, in principle, before any one of the layers of my CNN) and to operate feature extraction by fitting a **linear SVM** on top of these features (similar, in principle, to re-training on the last layer only, but with different loss). This approach has been shown to be much more robust to small dataset size.\n",
    "\n",
    "#### Re-use the same model on MNIST\n",
    "\n",
    "Basically, we wish to do this:\n",
    "\n",
    "![](img/tl1.jpg)\n",
    "\n",
    "We just need to import MNIST."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    "_, _, mnist_train, mnist_test = mnist.get_data()\n",
    "mnist_train.data.shape"
   ]
  },
  {
   "source": [
    "Have a look at the shape.\n",
    "\n",
    "**Q**: What do we need to do?\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_3(torch.utils.data.Dataset):\n",
    "    def __init__(self, mnist_dataset, transform=None):\n",
    "        n, h, w = mnist_dataset.data.shape\n",
    "        self.data = mnist_dataset.data.clone().unsqueeze(1).expand(n, 3, h, w)\n",
    "        self.targets = mnist_dataset.targets.clone()\n",
    "        if transform is None:\n",
    "            self.transform = lambda x: x\n",
    "        else:\n",
    "            self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.transform(self.data[index]), self.targets[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_mnist = T.Compose([\n",
    "    lambda x: x/255,\n",
    "    T.Normalize([0.1307, 0.1307, 0.1307], [0.3081, 0.3081, 0.3081])\n",
    "])\n",
    "mnist3_train = MNIST_3(mnist_train, transforms_mnist)\n",
    "mnist3_test = MNIST_3(mnist_test, transforms_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb2b7f4a820>"
      ]
     },
     "metadata": {},
     "execution_count": 116
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-13T16:19:20.062919</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 251.565 248.518125 \nL 251.565 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \nL 244.365 7.2 \nL 26.925 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p66ca645494)\">\n    <image height=\"218\" id=\"imagef7ff03c4e9\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\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\" y=\"-6.64\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m16852ec0dd\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#m16852ec0dd\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#m16852ec0dd\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#m16852ec0dd\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#m16852ec0dd\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#m16852ec0dd\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#m16852ec0dd\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mfe0f81c0d9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mfe0f81c0d9\" y=\"11.082857\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mfe0f81c0d9\" y=\"49.911429\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5 -->\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mfe0f81c0d9\" y=\"88.74\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mfe0f81c0d9\" y=\"127.568571\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 15 -->\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mfe0f81c0d9\" y=\"166.397143\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mfe0f81c0d9\" y=\"205.225714\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 25 -->\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 224.64 \nL 26.925 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 244.365 224.64 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 7.2 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p66ca645494\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.imshow(mnist3_train.data[0].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainloader = DataLoader(mnist3_train, 512, shuffle=FalAdamse, num_workers=0)\n",
    "mnist_testloader = DataLoader(mnist3_test, 512, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TESTING - loss -- - performance 0.8295833333333333\n",
      "TESTING - loss -- - performance 0.8285\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(None, 0.8285)"
      ]
     },
     "metadata": {},
     "execution_count": 118
    }
   ],
   "source": [
    "train.test_model(net, mnist_trainloader)\n",
    "train.test_model(net, mnist_testloader)"
   ]
  },
  {
   "source": [
    "We see that the performance is *kinda good*, but we'd expect more out of a model on MNIST.\n",
    "\n",
    "#### Feature extraction\n",
    "\n",
    "Basically we do this:\n",
    "\n",
    "![](img/tl2.jpg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "What we need to do:\n",
    "\n",
    "1. Freeze the training in the layers from the first to the second-to-last by *inhibiting* the `grad` in these layers\n",
    "2. Create the optimizer passing only the parameters of the last layer\n",
    "\n",
    "Notice that, if we consider the `named_parameters` of our net, the weight and bias of the last layer share the name `classifier.1`. Knowing that, we can inhibit the gradient by setting `param.requires_grad` to `False`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, pars in net.named_parameters():\n",
    "    if \"classifier.1\" not in name:\n",
    "        pars.requires_grad = False"
   ]
  },
  {
   "source": [
    "Mirroring the strategy above, we can *filter* the parameters being trained by the optimizer with a list comprehension:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 --- learning rate 0.10000\n",
      "Epoch 1 completed. Loss - total: 3036.26459813118 - average: 0.050604409968852994; Performance: 0.9852333333333333\n",
      "Epoch 2 --- learning rate 0.10000\n",
      "Epoch 2 completed. Loss - total: 3002.128536462784 - average: 0.05003547560771306; Performance: 0.98565\n",
      "Epoch 3 --- learning rate 0.10000\n",
      "Epoch 3 completed. Loss - total: 3000.8925540447235 - average: 0.050014875900745395; Performance: 0.9860166666666667\n",
      "Epoch 4 --- learning rate 0.10000\n",
      "Epoch 4 completed. Loss - total: 3009.51358294487 - average: 0.050158559715747834; Performance: 0.9862666666666666\n",
      "Epoch 5 --- learning rate 0.10000\n",
      "Epoch 5 completed. Loss - total: 3019.4148213863373 - average: 0.050323580356438954; Performance: 0.9863\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3019.4148213863373, 0.9863)"
      ]
     },
     "metadata": {},
     "execution_count": 126
    }
   ],
   "source": [
    "optim_finetune = torch.optim.SGD([pars for name, pars in net.named_parameters() if \"classifier.1\" in name], lr=.1, momentum=.9, weight_decay=5e-4)\n",
    "train.train_model(net, mnist_trainloader, nn.CrossEntropyLoss(), optim_finetune, 5)"
   ]
  },
  {
   "source": [
    "### Proper finetuning (ILSVRC2012 → CIFAR10)\n",
    "\n",
    "`torchvision` enables us to directly download pre-trained models from the web. Normally, we can access the weights of a large array of CNNs pretrained on the large-scale image classification dataset `ILSVRC2012`, also known as ImageNet, although the proper ImageNet dataset is a larger version of `ILSVRC2012`.\n",
    "\n",
    "These CNNs have the last layer whose output size is 1000 (the number of classes in `ILSVRC2012`).\n",
    "\n",
    "We wish to fine-tune a pre-trained `vgg11` model on the CIFAR10 dataset.\n",
    "\n",
    "This dataset is comparable in size to MNIST, with 10 categories:\n",
    "\n",
    "![](https://user-images.githubusercontent.com/16641054/46775076-8b17e480-cd40-11e8-9501-89c6fbca36bd.jpg)\n",
    "\n",
    "The images are slightly larger ($32 \\times 32$ instead of $28 \\times 28$) and are encoded as RGB (→ 3 channels).\n",
    "\n",
    "Let's import the dataset and create the DataLoaders"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using downloaded and verified file: datasets/CIFAR/cifar-10-python.tar.gz\n",
      "Extracting datasets/CIFAR/cifar-10-python.tar.gz to datasets/CIFAR\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_cifar = T.Compose([T.ToTensor(), T.Normalize([0.4913997551666284, 0.48215855929893703, 0.4465309133731618], [0.24703225141799082, 0.24348516474564, 0.26158783926049628])])\n",
    "\n",
    "cifar_train = CIFAR10(\"datasets/CIFAR\", train=True, transform=transform_cifar, download=True)\n",
    "cifar_test = CIFAR10(\"datasets/CIFAR\", train=False, transform=transform_cifar, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_trainloader = DataLoader(cifar_train, batch_size=128, shuffle=True)\n",
    "cifar_testloader = DataLoader(cifar_test, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of trainset (50000, 32, 32, 3)\nNum classes 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of trainset\", cifar_train.data.shape)\n",
    "print(\"Num classes\", max(cifar_test.targets) + 1)"
   ]
  },
  {
   "source": [
    "#### VGG11 \n",
    "\n",
    "VGG11 is a network similar to the CNN we created before:\n",
    "\n",
    "![](https://www.researchgate.net/publication/336550999/figure/fig1/AS:814110748987402@1571110536721/Figure-S1-Block-diagram-of-the-VGG11-architecture-Adapted-from-https-bitly-2ksX5Eq.png)\n",
    "\n",
    "In addition to the picture above, we also use batchnorm (at the time of the invention of VGG, batchnorm wasn't known)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg11_bn-6002323d.pth\" to /home/zullich/.cache/torch/hub/checkpoints/vgg11_bn-6002323d.pth\n",
      "4.6%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "13.5%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "22.5%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "31.2%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "39.8%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "48.7%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "57.4%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "66.3%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "74.8%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "84.1%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "93.2%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vgg = vgg11_bn(pretrained=True)\n",
    "vgg.classifier[-1].out_features = 10 # update the num_classes to reflect the new dataset"
   ]
  },
  {
   "source": [
    "Let's see how the network performs without fine-tuning:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TESTING - loss -- - performance 0.0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(None, 0.0)"
      ]
     },
     "metadata": {},
     "execution_count": 129
    }
   ],
   "source": [
    "train.test_model(vgg, cifar_testloader)"
   ]
  },
  {
   "source": [
    "In the next cells, you'll find how to fine-tune the CNN. It takes around 10 mins on single GPU, so I leave the commands and output as markdown notes.\n",
    "\n",
    "You can find the weights under the `models_push/vgg11_cifar` folder.\n",
    "\n",
    "Then, I'll also show how to train the `VGG11_bn` from scratch and leave the output of the training. You can clearly notice how the performance of the fine-tuned model is much better after 10 epochs (actually, this is not shown in the output, but the performance at epoch 1 is much higher in the pre-trained model).\n",
    "\n",
    "With proper data augmentation and lr scheduling (and maybe a deeper CNN), we can actually reach test-set accuracy of 94% or higher."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```\n",
    "optimizer = torch.optim.Adam(vgg.parameters())\n",
    "train.train_model(vgg, cifar_trainloader, nn.CrossEntropyLoss(), optimizer, 20)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```\n",
    "Epoch 10 completed. Loss - total: 5852.242087006569 - average: 0.11704484174013138; Performance: 0.96644\n",
    "\n",
    "Epoch 11 completed. Loss - total: 5551.681170940399 - average: 0.11103362341880799; Performance: 0.9682\n",
    "\n",
    "Epoch 12 completed. Loss - total: 5018.1823744773865 - average: 0.10036364748954774; Performance: 0.9718\n",
    "\n",
    "Epoch 13 completed. Loss - total: 17709.622314095497 - average: 0.35419244628190993; Performance: 0.92888\n",
    "\n",
    "Epoch 14 completed. Loss - total: 17280.205318450928 - average: 0.34560410636901856; Performance: 0.89974\n",
    "\n",
    "Epoch 15 completed. Loss - total: 3871.2677970528603 - average: 0.0774253559410572; Performance: 0.97692\n",
    "\n",
    "Epoch 16 completed. Loss - total: 2241.83546949178 - average: 0.0448367093898356; Performance: 0.9872\n",
    "\n",
    "Epoch 17 completed. Loss - total: 2088.5263759568334 - average: 0.041770527519136666; Performance: 0.9882\n",
    "\n",
    "Epoch 18 completed. Loss - total: 2305.237785771489 - average: 0.04610475571542978; Performance: 0.98758\n",
    "\n",
    "Epoch 19 completed. Loss - total: 2979.0519604235888 - average: 0.05958103920847178; Performance: 0.98364\n",
    "\n",
    "Epoch 20 completed. Loss - total: 3053.6747498363256 - average: 0.06107349499672651; Performance: 0.98368\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```\n",
    "train.test_model(vgg, cifar_testloader)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```\n",
    "TESTING - loss -- - performance 0.8552\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```\n",
    "torch.save(vgg.state_dict, \"models/vgg11_cifar/vgg11_tl.pt\")\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```\n",
    "vgg2 = vgg11_bn(pretrained=False, num_classes=10)\n",
    "optimizer = torch.optim.Adam(vgg2.parameters())\n",
    "train.train_model(vgg2, cifar_trainloader, nn.CrossEntropyLoss(), optimizer, 20)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```\n",
    "Epoch 10 completed. Loss - total: 48581.04235076904 - average: 0.9716208470153809; Performance: 0.66648\n",
    "\n",
    "Epoch 11 completed. Loss - total: 39186.03456878662 - average: 0.7837206913757324; Performance: 0.72996\n",
    "\n",
    "Epoch 12 completed. Loss - total: 35287.52504825592 - average: 0.7057505009651184; Performance: 0.76124\n",
    "\n",
    "Epoch 13 completed. Loss - total: 38614.65808200836 - average: 0.7722931616401673; Performance: 0.7412\n",
    "\n",
    "Epoch 14 completed. Loss - total: 30812.80309009552 - average: 0.6162560618019104; Performance: 0.79158\n",
    "\n",
    "Epoch 15 completed. Loss - total: 30947.483282089233 - average: 0.6189496656417847; Performance: 0.79536\n",
    "\n",
    "Epoch 16 completed. Loss - total: 30550.97905731201 - average: 0.6110195811462402; Performance: 0.80206\n",
    "\n",
    "Epoch 17 completed. Loss - total: 23598.16388607025 - average: 0.471963277721405; Performance: 0.85028\n",
    "\n",
    "Epoch 18 completed. Loss - total: 32089.43673324585 - average: 0.641788734664917; Performance: 0.80358\n",
    "\n",
    "Epoch 19 completed. Loss - total: 37266.2276263237 - average: 0.745324552526474; Performance: 0.78556\n",
    "\n",
    "Epoch 20 completed. Loss - total: 23250.735929965973 - average: 0.4650147185993195; Performance: 0.84466\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```\n",
    "train.test_model(vgg2, cifar_testloader)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```\n",
    "TESTING - loss -- - performance 0.7927\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```\n",
    "torch.save(vgg.state_dict(), \"models/vgg11_cifar/vgg11_scratch.pt\")\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### ResNets\n",
    "\n",
    "As told before, the building block of resnets is the **residual block**, which enables the network to train faster and avoid the vanishing gradient problem:\n",
    "\n",
    "![](https://www.aiuai.cn/uploads/sina/5ce8dfe46e373.jpg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "From the image above, we see two popular implementations of the residual block.\n",
    "\n",
    "* the first implementation is the simplest one. We have two $3\\times 3$, 1-padding, convolutions and the same number of filters as the incoming data.\n",
    "\n",
    " We can obtain the spatial dimension of the output (i.e., height and width) with this formula (found in [1]): $\\text{output_size} = \\text{in_size} + 2\\cdot \\text{pad} - \\text{ker_size} + 1$. Since $p=1,~\\text{pad}=2,~\\text{ker_size}=3$, then $\\text{output_size} = \\text{in_size}$. Since we do not vary the number of channels in the data, we can safely sum the incoming data to the output of the second convolutional layer.\n",
    "    * In some layers (architecture-dependent) a *subsampling* may be required (i.e., we need to decrease the spatial dimensions of the data). This is solved by applying a convolution with stride of 2 (the moving window *shifts* by 2 pixels instead of 1).\n",
    "     \n",
    "     In this case, **we need to downsample the data in the skip connections** as well. This is done via **$1\\times 1$ convolutions with stride of 2**.\n",
    "\n",
    " Let's implement this:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, n_channels, activ=nn.ReLU, bias=False, batchnorm=True, downsample=False):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=n_channels, kernel_size=3, padding=1, bias=bias, stride=(2 if downsample else 1))\n",
    "        self.bnorm1 = nn.BatchNorm2d(n_channels)\n",
    "        self.activ1 = activ()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=3, padding=1, bias=bias)\n",
    "        self.bnorm2 = nn.BatchNorm2d(n_channels)\n",
    "        self.activ2 = activ()\n",
    "\n",
    "        if downsample:\n",
    "            self.conv_down = nn.Conv2d(in_channels=in_channels, out_channels=n_channels, kernel_size=1, stride=2, padding=1, bias=bias)\n",
    "            self.bnorm_down = nn.BatchNorm2d(n_channels) # note that this is necessary in order to put data in the skip connection and data in the regular stream on the same domain!\n",
    "            \n",
    "    def forward(self, X):\n",
    "        out = self.activ1(self.bnorm1(self.conv1(X)))\n",
    "        # let us complete this\n",
    "        pass"
   ]
  },
  {
   "source": [
    "* the second implementation is also called a **bottleneck layer** and operates an expansion of the number of channels (in a fashion similar to VGG). Differently from the residual block:\n",
    "    * the bottleneck has 3 instead of 2 convolutional layers\n",
    "        * the 1st and 3rd layer operate a $1\\times 1$ correlation with no padding (thus, leaving the image untouched in size)\n",
    "        * the 2nd is a regular $3\\times 3$ correlation which we saw before (padding of 1); however this can be a \"**dilated convolution**\":\n",
    "         ![](https://www.researchgate.net/publication/320195101/figure/fig2/AS:669211164692494@1536563783748/Dilated-convolution-On-the-left-we-have-the-dilated-convolution-with-dilation-rate-r.png)\n",
    "    * the width of the convolutional layers is controlled by a parameter (it should be $\\geq$ than the width of the incoming data -- canonical values are 64, 128...)\n",
    "    * the output of the block has a number of channels which is usually $\\times 4$ the width of the block.\n",
    "    * if a **downsampling** is required, it's operated by setting the stride to 2 for the second convolutional layer only."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, in_channels, n_channels, activ=nn.ReLU, bias=False, batchnorm=True, dilation_rate=1, expansion_factor=4, downsample=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=n_channels, kernel_size=1, padding=0, bias=bias)\n",
    "        self.bnorm1 = nn.BatchNorm2d(n_channels)\n",
    "        self.activ1 = activ()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=3, padding=1, bias=bias, dilation=dilation_rate,  stride=(2 if downsample else 1))\n",
    "        self.bnorm2 = nn.BatchNorm2d(n_channels)\n",
    "        self.activ2 = activ()\n",
    "\n",
    "        out_channels = n_channels * expansion_factor\n",
    "        self.conv3 = nn.Conv2d(in_channels=n_channels, out_channels=out_channels, kernel_size=1, padding=0, bias=bias)\n",
    "        self.bnorm3 = nn.BatchNorm2d(out_channels)\n",
    "        self.activ3 = activ()\n",
    "\n",
    "    def forward(self, X):\n",
    "        # this is wrong and serves just as a way to check the dims of the output\n",
    "        out = self.activ1(self.bnorm1(self.conv1(X)))\n",
    "        print(out.shape)\n",
    "        out = self.activ2(self.bnorm2(self.conv2(out)))\n",
    "        print(out.shape)\n",
    "        out = self.activ3(self.bnorm3(self.conv3(out)))\n",
    "        print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    BottleneckBlock(64, 64),\n",
    "    BottleneckBlock(256, 64),\n",
    "    BottleneckBlock(256, 128)\n",
    ")\n",
    "X = torch.rand((24, 64, 100, 100))\n",
    "_ = net(X)"
   ]
  },
  {
   "source": [
    "**Q**: we're missing a step to \"close the loop\" on the skip connection. What is it?\n",
    "\n",
    "*Hint*: have a look at the shapes of the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Resnet18\n",
    "\n",
    "Resnet18 is the simplest resnet architecture (18 total layers) presented in [2]. Let's see how we can implement it:\n",
    "\n",
    "![](https://pytorch.org/assets/images/resnet.png)\n",
    "\n",
    "From the image above, we can see that the Resnet18 is composed of one initial convolution (preparatory layer), four residual blocks, and one flattening/classification block."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "##### Preparatory layer\n",
    "\n",
    "Is a $7\\times 7$ convolution with stride 2 and 64 output channels, followed by batchnorm, relu, and maxpooling with a $3\\times 3$ kernel (!) and stride 2.\n",
    "\n",
    "Its function is mainly to reduce the image in size while extracting a large array of low-level features.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetPrep(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=64, activ=nn.ReLU, bias=False):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=7, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            activ(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "source": [
    "##### Residual blocks\n",
    "\n",
    "We have eight residual blocks with increasing number of channels, VGG-style (64,64→128,128→256,256→512,512).\n",
    "\n",
    "Whenever the number of channels is increased, we have also a subsampling.\n",
    "\n",
    "##### Average pooling and classification\n",
    "\n",
    "We flatten the array with \"adaptive average pooling\", which is a nice way to say that we're averaging across width.\n",
    "\n",
    "Now, we can start preparing our Resnet18 class."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10, in_channels=3, base_width=64, activ=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.prep = ResNetPrep(in_channels=in_channels, out_channels=base_width, activ=activ)\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            ResidualBlock(in_channels=base_width, n_channels=base_width, activ=activ),\n",
    "            ResidualBlock(in_channels=base_width, n_channels=base_width, activ=activ),\n",
    "\n",
    "            ResidualBlock(in_channels=base_width, n_channels=base_width * 2, activ=activ, downsample=True),\n",
    "            ResidualBlock(in_channels=base_width * 2, n_channels=base_width * 2, activ=activ),\n",
    "\n",
    "            ResidualBlock(in_channels=base_width * 2, n_channels=base_width * 4, activ=activ, downsample=True),\n",
    "            ResidualBlock(in_channels=base_width * 4, n_channels=base_width * 4, activ=activ),\n",
    "\n",
    "            ResidualBlock(in_channels=base_width * 4, n_channels=base_width * 8, activ=activ, downsample=True),\n",
    "            ResidualBlock(in_channels=base_width * 8, n_channels=base_width * 8, activ=activ)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.classifier = nn.Linear(in_features=base_width*8, out_features=num_classes, bias=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = self.prep(X)\n",
    "        out = self.res_blocks(out)\n",
    "        out = self.classifier(self.avgpool(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\n├─ResNetPrep: 1-1                        --\n|    └─Sequential: 2-1                   --\n|    |    └─Conv2d: 3-1                  9,408\n|    |    └─BatchNorm2d: 3-2             128\n|    |    └─ReLU: 3-3                    --\n|    |    └─MaxPool2d: 3-4               --\n├─Sequential: 1-2                        --\n|    └─ResidualBlock: 2-2                --\n|    |    └─Conv2d: 3-5                  36,864\n|    |    └─BatchNorm2d: 3-6             128\n|    |    └─ReLU: 3-7                    --\n|    |    └─Conv2d: 3-8                  36,864\n|    |    └─BatchNorm2d: 3-9             128\n|    |    └─ReLU: 3-10                   --\n|    └─ResidualBlock: 2-3                --\n|    |    └─Conv2d: 3-11                 36,864\n|    |    └─BatchNorm2d: 3-12            128\n|    |    └─ReLU: 3-13                   --\n|    |    └─Conv2d: 3-14                 36,864\n|    |    └─BatchNorm2d: 3-15            128\n|    |    └─ReLU: 3-16                   --\n|    └─ResidualBlock: 2-4                --\n|    |    └─Conv2d: 3-17                 73,728\n|    |    └─BatchNorm2d: 3-18            256\n|    |    └─ReLU: 3-19                   --\n|    |    └─Conv2d: 3-20                 147,456\n|    |    └─BatchNorm2d: 3-21            256\n|    |    └─ReLU: 3-22                   --\n|    └─ResidualBlock: 2-5                --\n|    |    └─Conv2d: 3-23                 147,456\n|    |    └─BatchNorm2d: 3-24            256\n|    |    └─ReLU: 3-25                   --\n|    |    └─Conv2d: 3-26                 147,456\n|    |    └─BatchNorm2d: 3-27            256\n|    |    └─ReLU: 3-28                   --\n|    └─ResidualBlock: 2-6                --\n|    |    └─Conv2d: 3-29                 294,912\n|    |    └─BatchNorm2d: 3-30            512\n|    |    └─ReLU: 3-31                   --\n|    |    └─Conv2d: 3-32                 589,824\n|    |    └─BatchNorm2d: 3-33            512\n|    |    └─ReLU: 3-34                   --\n|    └─ResidualBlock: 2-7                --\n|    |    └─Conv2d: 3-35                 589,824\n|    |    └─BatchNorm2d: 3-36            512\n|    |    └─ReLU: 3-37                   --\n|    |    └─Conv2d: 3-38                 589,824\n|    |    └─BatchNorm2d: 3-39            512\n|    |    └─ReLU: 3-40                   --\n|    └─ResidualBlock: 2-8                --\n|    |    └─Conv2d: 3-41                 1,179,648\n|    |    └─BatchNorm2d: 3-42            1,024\n|    |    └─ReLU: 3-43                   --\n|    |    └─Conv2d: 3-44                 2,359,296\n|    |    └─BatchNorm2d: 3-45            1,024\n|    |    └─ReLU: 3-46                   --\n|    └─ResidualBlock: 2-9                --\n|    |    └─Conv2d: 3-47                 2,359,296\n|    |    └─BatchNorm2d: 3-48            1,024\n|    |    └─ReLU: 3-49                   --\n|    |    └─Conv2d: 3-50                 2,359,296\n|    |    └─BatchNorm2d: 3-51            1,024\n|    |    └─ReLU: 3-52                   --\n├─AdaptiveAvgPool2d: 1-3                 --\n├─Linear: 1-4                            5,130\n=================================================================\nTotal params: 11,007,818\nTrainable params: 11,007,818\nNon-trainable params: 0\n=================================================================\n"
     ]
    }
   ],
   "source": [
    "_ = summary(ResNet18())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ResNet18(\n",
       "  (prep): ResNetPrep(\n",
       "    (layers): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (res_blocks): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ1): ReLU()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ2): ReLU()\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ1): ReLU()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ2): ReLU()\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bnorm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ1): ReLU()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ2): ReLU()\n",
       "      (conv_down): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bnorm_down): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ1): ReLU()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ2): ReLU()\n",
       "    )\n",
       "    (4): ResidualBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bnorm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ1): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ2): ReLU()\n",
       "      (conv_down): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bnorm_down): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ1): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ2): ReLU()\n",
       "    )\n",
       "    (6): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bnorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ1): ReLU()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ2): ReLU()\n",
       "      (conv_down): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bnorm_down): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): ResidualBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ1): ReLU()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "ResNet18()"
   ]
  },
  {
   "source": [
    "### Deconvolution\n",
    "\n",
    "Deconvolution (*inverse convolution, transposed convolution*) is a process analogous to correlation. Usually, given a generic square matrix $M\\in \\mathbb{R}^{n\\times n}$ and kernel $K\\in \\mathbb{R}^{k\\times k},~k<n$, we have:\n",
    "\n",
    "$M \\otimes K = L$, with $L\\in\\mathbb{R}^{(n-k+1)~\\times~(n-k+1)}$\n",
    "\n",
    "With the deconvolution we want to solve the \"inverse\" problem, i.e., given the same kernel $K$, from a smaller matrix $H$, obtain a larger matrix $N$.\n",
    "\n",
    "$H \\tilde\\otimes K = N$\n",
    "\n",
    "The first equation can be transformed into a matrix-matrix multiplication prolbem by reshaping the kernel in the following way:\n",
    "\n",
    "$\\mathcal{K} =\n",
    "\\begin{Bmatrix}\n",
    "k_{00} & k_{01} & k_{02} & 0 & k_{10} & k_{11} & k_{12} & 0 & k_{20} & k_{21} & k_{22} & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & k_{00} & k_{01} & k_{02} & 0 & k_{10} & k_{11} & k_{12} & 0 & k_{20} & k_{21} & k_{22} & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & k_{00} & k_{01} & k_{02} & 0 & k_{10} & k_{11} & k_{12} & 0 & k_{20} & k_{21} & k_{22} & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & k_{00} & k_{01} & k_{02} & 0 & k_{10} & k_{11} & k_{12} & 0 & k_{20} & k_{21} & k_{22} \\\\\n",
    "\\end{Bmatrix}\n",
    "$\n",
    "\n",
    "Then, the convolution can be rethought as:\n",
    "\n",
    "$ \\mathcal{K} \\cdot\\text{vec}(M) = \\text{vec}(L)$, with a later row-wise reshaping of $\\text{vec}(L)$\n",
    "\n",
    "Similarly, we can view the deconvolution problem as:\n",
    "\n",
    "$ \\mathcal{K}^\\top \\cdot\\text{vec}(H) = \\text{vec}(N)$\n",
    "\n",
    "dimensions: $(16\\times4) \\cdot (4\\times1) = (16\\times1)$\n",
    "\n",
    "Thus, by properly adjusting the weights $\\begin{Bmatrix}k_{00} & k_{01} & k_{02} & k_{10} & k_{11} & k_{12} & k_{20} & k_{21} & k_{22}\\end{Bmatrix}$, we may also be able to recover the matrix $M$ starting from $L$.\n",
    "\n",
    "#### Where is it used?\n",
    "\n",
    "Deconvolution is used in tasks requiring an \"intelligent upsampling\". A trivial upsampling can be achieved by using techniques like \n",
    "\n",
    "*max-unpooling*: ![](https://www.researchgate.net/publication/335754645/figure/fig4/AS:839690152329216@1577209141459/Example-of-an-unpooling-process-Indices-of-max-pooling-are-kept-up-and-reused-to.png)\n",
    "\n",
    "*regular convolution with a lot of padding*: ![](https://miro.medium.com/max/2588/1*uk4KJEtyDuPOipfG4yd-WA.gif)\n",
    "\n",
    "but the final result will be really unsatisfying.\n",
    "\n",
    "Upsampling may be required e.g. in tasks where the output size must be the same as the input size.\n",
    "\n",
    "This strategy is often employed for tasks of **semantic segmentation**\n",
    "\n",
    "![](https://i.pinimg.com/originals/62/d8/5a/62d85a6488ffa85bde7af3a9a8ed70ff.gif)\n",
    "\n",
    "The task can be summarized with this phrase: instead of trying to classify the whole image, we wish to classify each pixel within a large image.\n",
    "\n",
    "One of the most succesful CNNs for image segmentation is U-Net, which was originally proposed for medical imaging.\n",
    "\n",
    "![](img/unet_small.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, width):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, width, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(width, width, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, width, out_channels):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, width, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(width, width, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(width, out_channels, 2, stride=2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.down1 = DownBlock(in_channels, 64)\n",
    "        self.down2 = DownBlock(64, 128)\n",
    "        self.down3 = DownBlock(128, 256)\n",
    "        self.down4 = DownBlock(256, 512)\n",
    "        self.up1 = UpBlock(512, 1024, 512)\n",
    "        self.up2 = UpBlock(1024, 512, 256)\n",
    "        self.up3 = UpBlock(512, 256, 128)\n",
    "        self.up4 = UpBlock(256, 128, 64)\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 1, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        down_64 = self.down1(X)\n",
    "        print(down_64.shape)\n",
    "        down_128 = self.down2(down_64)\n",
    "        print(down_128.shape)\n",
    "        down_256 = self.down3(down_128)\n",
    "        print(down_256.shape)\n",
    "        down_512 = self.down4(down_256)\n",
    "        print(down_512.shape)\n",
    "        out = self.up1(down_512)\n",
    "        np.cat((down_512, out), dim=1)\n",
    "        out = self.up2(out)\n",
    "        np.cat((down_256, out), dim=1)\n",
    "        out = self.up3(out)\n",
    "        np.cat((down_128, out), dim=1)\n",
    "        out = self.up4(out)\n",
    "        np.cat((down_64, out), dim=1)\n",
    "        return self.final(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'UNet' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7fbaf49add23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m572\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m572\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'UNet' is not defined"
     ]
    }
   ],
   "source": [
    "net = UNet()\n",
    "x = torch.rand((24, 3, 572, 572))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a87f5dbd59bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mo1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mo1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "o1 = net.down1(x)\n",
    "o1.shape"
   ]
  },
  {
   "source": [
    "#### U-Net\n",
    "\n",
    "The architecture is a composition of two parts: a contracting module and an expanding module.\n",
    "\n",
    "The contracting module is a sequence of VGG convolutional blocks. \n",
    "\n",
    "After reaching the bottom part, we begin upsampling the image following the inverse of the scheme from the contracting module, with an additional operation: we concatenate the output of the upsampling with the output from the last convolutional layer of the corresponding block (as in the image). Thus, if the upsampling yields a 256-channel output, we concatenate this output with the output of the last 256-channel convolutional layer from the contracting module. This leaves us with a 512-channel tensor which we convolve to 256-channels once again. Note that, if the spatial dimensions of the data from the contracting module doesn't match those of the upsampled data, cropping is operated so that we can safely concatenate the two tensors.\n",
    "\n",
    "For what concerns the output, instead, we end up with a tensor of shape $h^\\prime \\times w^\\prime \\times C$, where $C$ denotes the number of the classes we want to operate segmentation (logically speaking, **if we want to classify each pixel, we wish to produce a softmax for each pixel**).\n",
    "\n",
    "![](img/unet_last.jpg)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Homework\n",
    "\n",
    "Taking inspiration from the picture above, implement a U-Net-style CNN with the following specs:\n",
    "\n",
    "1. All convolutions must use a $3\\times 3$ kernel and **leave the spatial dimensions (i.e. height, width) of the input untouched**.\n",
    "2. Downsampling in the contracting part is performed via maxpooling with a $2\\times 2$ kernel and stride of 2.\n",
    "3. Upsampling is operated by a deconvolution with a $2\\times 2$ kernel and stride of 2. The PyTorch module that implements the deconvolution is `nn.ConvTranspose2d`\n",
    "4. The final layer of the expanding part has only 1 channel \n",
    "    * between how many classes are we discriminating?\n",
    "\n",
    "Create a network class with (at least) a `__init__` and a `forward` method. Please resort to additional structures if you believe it helps readability of your code.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Trans"
   ]
  }
 ]
}